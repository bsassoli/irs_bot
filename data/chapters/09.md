## CHAPTER 9 Statistical reasoning

###  9.1 WORLD RELIGIONS AND TWO USES OF STATISTICS

After reading this section, you should be able to:

- Characterize what statistical information reveals about religious belief worldwide
- Describe what statistics is and what purposes it serves
- Indicate two differences between descriptive and inferential statistics

#### Is religion a thing of the past?

Do you believe religion is going to disappear in the near future? Do you believe Christians are less oppressed than other religious communities? Depending on where you live and what communities you participate in, you might believe one or both of these ideas. But the data indicate otherwise.

According to a series of studies conducted by the Pew Research Center, a nonpartisan policy institute based in Washington DC, all major religions around the world are currently growing and will continue growing in the future. As of 2022, compared to nonbelievers, religious people are generally younger, have more children, and live outside western Europe and North America. Atheists, agnostics, and others without any religious affiliation are indeed increasing in numbers in countries like France and the US, but the populations of those countries are a declining share of the world’s total population. Islam is growing faster than any other religion, though Christianity will remain the largest religion for the next few decades. There are now about 2.3 billion Christians, of which more than half are Catholics. And, though it may sound surprising, Christians are harassed and persecuted in more countries than any other religious community.

If you wonder how we can possibly know all of this, the answer is that we know based on statistical evidence.

Up until the 18th century, statistics referred to any data relevant to running a state, like data about birth and death rates, the size of different religious groups, individual and national wealth, the level of employment, and the share of the population eligible for military service. Today, statistics is a set of tools broadly used in science to

##### Size and projected growth of major religious groups

| Religious Group | 2010 Population | % of World Population in 2010 | Projected 2050 Population | % of World Population in 2050 | Growth 2010–2050 |
| --------------- | --------------- | ----------------------------- | ------------------------- | ----------------------------- | ---------------- |
| Christians      | 2,168,330,000   | 31.4                          | 2,918,070,000             | 31.4                          | 749,740,000      |
| Muslims         | 1,599,700,000   | 23.2                          | 2,761,480,000             | 29.7                          | 1,161,780,000    |
| Unaffiliated    | 1,131,150,000   | 16.4                          | 1,230,340,000             | 13.2                          | 99,190,000       |
| Hindus          | 1,032,210,000   | 15.0                          | 1,384,360,000             | 14.9                          | 352,140,000      |
| Buddhists       | 487,760,000     | 7.1                           | 486,270,000               | 5.2                           | −1,490,000       |
| Folk Religions  | 404,690,000     | 5.9                           | 449,140,000               | 4.8                           | 44,450,000       |
| Other Religions | 58,150,000      | 0.8                           | 61,450,000                | 0.7                           | 3,300,000        |
| Jews            | 13,860,000      | 0.2                           | 16,090,000                | 0.2                           | 2,230,000        |
| World total     | 6,895,850,000   | 100.0                         | 9,307,190,000             | 100.0                         | 2,411,340,000    |

Source: Pew Research Center

Systematically collect, curate, analyze, present, and interpret data. The data don’t have to be about human populations—they can be about literally anything targeted in scientific investigation, from subatomic particles to long-extinct species.

Sound statistical reasoning and the analysis of others’ statistical reasoning are incredibly important for knowing what to believe and why. This is even more crucial because scientists, journalists, politicians, pundits, and most people are sometimes sloppy, or even deliberately misleading, in how they use statistical methods and evidence. The ability to correctly assess statistical information (and misinformation too!) is crucial.

#### Descriptive and inferential statistics

There are two main kinds of statistical reasoning. The first kind, the main focus of this chapter, is descriptive statistics: summarizing, describing, and displaying data in a meaningful way. Finding a class’s average score on an exam is a common use of descriptive statistics. Finding patterns in a data set—such as averages, trends, and correlations—and graphically representing them are forms of descriptive statistics with many applications, from scientific research to business and politics.

The second kind of statistical reasoning, the topic of Chapter 10, is inferential statistics, or using statistical reasoning to draw broader conclusions on the basis of limited data. This kind of statistical reasoning is used to make inferences from a dataset that “go beyond” that data set to capture features of the broader phenomenon the data represent. You may use tools of inferential statistics to estimate the distribution of grades you would expect in your whole course based on the exam grades from only some students or to predict the grades of students in other sections of the course from the grades in one section. We saw in Chapter 7 that drawing conclusions that “go beyond” what is known is a feature of inductive inference, where the relationship between premises and conclusion is one of probability rather than certainty or necessity.

This is true for inferential statistics as well. In fact, inferential statistics is also known as inductive statistics.

To illustrate the distinction between descriptive and inferential statistics, consider our example of statistics about religious belief. Religiosity is a variable that can take different values: Hindu, Jewish, Christian, Muslim, Rastafarian, atheist, and so on. If you survey a group of people to measure that variable, you will probably find some variation in their answers. Some will tell you they are Hindus, others that they are atheists, others that they are Muslim, and yet others may prefer not to disclose that information.

Simple tools of descriptive statistics enable you to summarize with percentages the share of each value of the variable Religiosity for the group you surveyed. Suppose, for example, that you poll 30 people in your town or at your university, and you find that four are Jews, eleven Rastafarian, five atheists, two Buddhists, one does not say, and seven are Catholics. This means that five people among the 30 you polled are atheists, or 1/6 (16.6%) of your total sample. It also means the most common religion in your data set is Rastafarian. There are, of course, other conclusions you could draw. You may decide to summarize the data you collected graphically, perhaps in a pie chart, to make understandable at a glance how religiosity varies across the people you polled. You may also want to investigate the relationship between religiosity and other variables like age in the same group of 30 people; you may discover that almost all the seven Catholics among the people you polled are over 40 years old.

But many times when we conduct a survey, we are not really interested in the particular people we survey. Instead, we want to draw broader conclusions, about religiosity, for instance, on the basis of surveying a sample of people. You might want to draw conclusions about religious belief in your town or even across the world’s population. Scientists, policymakers, and business analysts also regularly aim to generalize from the groups they study to a larger group, which they call a population.

In statistics, a population need not be the world population nor even a human population. A population is a large collection of entities that shares some characteristic. Some researchers are interested in populations of humans, but others study populations of bacteria, stars, subatomic particles, or more abstract objects like companies, households, and nations. In most cases, it is impossible to collect data about each individual in a population—think about surveying all the people in India on some question or collecting data about all stars of the Milky Way galaxy. For this reason, scientists regularly use inferential statistics to draw conclusions about a population on the basis of data about a subset of the population, or sample.

Researchers on world religions rely on census data, as well as polling, phone canvassing, media content analyses, and other sample data to answer general questions about the world population like: How many Hindus are there now? Is the number of nonreligious people growing? Will Catholics outnumber Evangelicals in 50 years in Brazil? These and other questions can be answered with the help of tools from inferential statistics. The answers can then be displayed with percentages, or graphically with charts and plots, using tools from descriptive statistics. The information on world religions we began the chapter with is based on inferential statistics. Instead of simply summarizing data, inferential statistics uses the data as evidence relevant to evaluating hypotheses about the larger population. This evidence is defeasible and is open to criticism and revision, just like any other type of evidence in science and everyday life.

A second difference between descriptive and inferential statistics concerns the kinds of error each involves, where error refers to the difference between the values recorded in a data set and the true value of the variable one is measuring. Both descriptive and inferential statistics involve measurement error, since all measurements are limited in precision and accuracy. Any time one tries to measure something—the weight of a person, the number of Buddhists in the world, or the economic performance of a company—the value obtained will vary across measurements, and most of the measurements will be slightly off the true value. For example, in polling 30 people about their religion, you might accidentally misreport one or two answers, or some of the people may lie or not to respond.

In contrast, only inferential statistics involves sampling error. Unlike descriptive statistics, inferential statistics is used to generalize from the observed sample to the mostly unobserved population.

#### Sampling error

is differences between the features of a sample and a population due to the unrepresentativeness of the sample. If the sample is not representative of the population you want to learn about due to sheer chance variation or nonrandom sampling, the conclusions you draw about a population from a sample may be incorrect. If you want to know about global trends in religiosity, for example, it’s probably a bad idea to rely solely on a sample within a particular country. Even a randomized worldwide survey can lead to results that vary from the population to some extent by sheer chance.

##### EXERCISES

9.1 Recall: List a few features of religion worldwide that we know through the use of inferential statistics.

9.2 Apply: Define sample and population. Then, state whether each of the following statements refers to a sample or to a population, briefly justifying your answer:

- a. Researchers found that 2% of the Americans they interviewed believed they had seen a UFO.
- b. Based on their survey data, the researchers concluded that 1 in 3 of all car crashes in the country are linked to alcohol impairment.
- c. Two thirds of the butterflies we observed were pink.
- d. After reading four essays, the teacher expects that 85% of the class will pass the exam.
- e. 25% of the planets in the solar system have no moons.
- f. More than one billion people in the world live on less than one dollar a day.

9.3 Recall: What are the two main differences between descriptive statistics and inferential statistics?

9.4 Apply: Indicate whether each of the following statements is based on descriptive or inferential statistics, and explain why.

1. As of 2023, the director Quentin Tarantino has received a total of two Academy Awards.
2. Students with an undergraduate GPA of 3.00 are expected to have a starting salary of $50,000.
3. In 2020, the population of São Paulo, Brazil, was 12.33 million.
4. The mean score of the class was B+.
5. A study stated that British adults are nearly 12 kilograms heavier now than they were in 1960.
6. Economists say that mortgage rates may soon drop.
7. The gross national income per capita in South Sudan in 2015 was $1,040.
8. According to the latest WHO data published in 2020, life expectancy in Bangladesh is 72 years.

9.5 Apply: Find a news article or opinion column published in the past week that uses statistical reasoning of some kind. After citing the source, write a paragraph describing:

1. The main point of the article or column
2. What statistics are provided
3. How the author makes use of the statistics in their reasoning
4. How good this use of statistical reasoning seems to be, and why

9.6 Think: Statistical reasoning pervades our lives, often in ways we don’t realize. After reflecting on your daily routine, write out a list of four ways in which statistical reasoning is part of that routine, either explicitly or implicitly. In light of your examples, briefly explain how incorrect statistical reasoning could make a concrete difference to your life.

### 9.2 STATISTICAL DISTRIBUTION AND CORRELATION

After reading this section, you should be able to:

- Identify and evaluate pie charts, bar charts, histograms, and scatterplots
- Characterize different types of statistical distributions
- Define statistical independence and correlated variables and evaluate direction and strength of correlations from a scatterplot, regression analysis, or correlation coefficient

#### Visualizing statistical distributions

As mentioned in section 9.1, descriptive statistics involves summarizing, describing, and displaying data in a meaningful way. As a reminder, variables are anything that can vary, change, or occur in different states and that can be measured—that is, that take on different values. Random variables are variables whose values are...

#### Statistical reasoning

Individually unpredictable but predictable in the aggregate. Descriptive statistics can display such aggregate information. For instance, while you can’t guess the religious belief of someone you don’t know, you can use statistical data to describe what religions are most common and how religions vary in their distribution across the world.

Charts, tables, and graphs provide visual representations of the statistical features of random variables. This is a common form of descriptive statistics in scientific research, as well as in newspapers and magazines. With the graphical representation of statistics, the key to success is a simple, clear, and appropriate presentation of the data. Different kinds of visual representations are helpful in different circumstances, depending in part on the kind of data and the needs of the audience.

Consider pie charts, in which a circle is divided into different-sized slices to depict how much of the outcome space for a variable falls into different categories of values. The area of each slice represents the percentage of outcomes associated with that value of the variable: the bigger the percentage, the bigger the area. To avoid confusion, there shouldn’t be too many categories shown in a pie chart, and the values of the variable those categories depict should be mutually exclusive and collectively exhaustive. This is needed so that the pie slices don’t overlap with one another (mutually exclusive) and so they add up to a whole pie (collectively exhaustive). A pie chart is a useful way to represent qualitative variables—variables with values that are not numerical but descriptive, like the variable sport, with values basketball, hockey, and so on. For example, a pie chart can be used to represent the percentage of a coffee shop’s sales for different beverages—say, americanos, cortados, cappuccinos, chai tea lattes, espresso macchiatos, and smoothies. Notice that if this list doesn’t include every item on the menu, then there needs to be an “other” category as well so the values are collectively exhaustive.

Pie charts are most effective when representing a variable that can take a small number of distinct values. The values must be distinct, so you know where to draw the lines between them, and too many narrow slices makes a pie chart difficult to read and understand. Besides coffee shop beverages, pie charts also can be used to display the distribution of votes received by candidates in some election or the distribution of quiz grades earned by students. Determining the proper variable can sometimes be tricky; in the latter example, the variable is not student but grade (outcome space = {A, B, C, D, F}). This also shows how it can be necessary to group values into broad groupings: a pie chart of percentage grades would have too many categories for an effective pie chart, while letter grades are broad enough groupings to be depicted in this way.

Bar charts use bars of different heights to show the amount for different values of some variable. The values are typically placed horizontally and equally spaced; then vertical bars are used to represent the size of each value. This size can correspond to an absolute number, like the number of students who got an A, B, C, D, or F on the most recent test, or a percentage, like the percent of the class who got each grade. Like pie charts, bar charts are great for categorical variables that can take discrete values. The values should still be mutually exclusive, but they don’t need to be collectively exhaustive. (For a pie chart, the categories have to add up to a whole pie, but bar charts are not limited in this way.) Bar charts are better than pie charts for comparison of proportion between values or when there is a larger number of values. Figure 9.3 shows an example bar chart of per capita (per person) beer consumption across several countries. This wouldn’t work as a pie chart, since the values aren’t collectively exhaustive: more beer than this is sold in the world, and also each value is a per-person average rather than a national total.

##### Table 9.2 Coﬀee Shop Sales

| cappuccinos     | 24%  |
| --------------- | ---- |
| smoothies       | 7%   |
| chai tea lattes | 11%  |
| americanos      | 19%  |
| cortados        | 3%   |

Like bar charts, histograms are graphical displays of data that use bars of different heights. Unlike bar charts, the bars of a histogram are not distinct categories. Instead, the values are grouped into numeric intervals. Histograms are effective ways to visually depict values of quantitative variables—variables with numerical values, such as height or percent correct on an exam—that don’t obviously fall into discrete categories. One example of such a variable is the height of students in your class. People’s height varies continuously. If you develop a histogram of students’ height, it is up to you to decide what numeric intervals to use to group the continuous variable of height—for example, you might group together all heights within 1-foot (30.48-centimeter) intervals or only within 10-centimeter intervals. Your decision will partly depend on the range of values, that is, the difference between the largest and smallest values in the data set. If everyone in your class is between 5 and 6 feet tall, grouping that full interval together will result in an uninformative histogram. Like bar charts, bar height in histograms may reflect the total number in each interval or their percentage.

##### Table 9.3

| Example bar chart showing beer consumption per capita for several nations | Example bar chart showing beer consumption per capita for several nations |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 200.0                                                        | 181.7                                                        |
| 180.0                                                        |                                                              |
| 160.0                                                        |                                                              |
| 140.0                                                        |                                                              |
| 120.0                                                        |                                                              |
| 100.0                                                        |                                                              |
| 80.0                                                         | 84.4                                                         |
| 60.0                                                         | 83.4                                                         |
| 40.0                                                         | 84.4                                                         |
| 20.0                                                         | 82.2                                                         |
| 0.0                                                          | 81.6                                                         |



Unlike bar charts, bars aren’t necessary for a histogram. Histograms may use other approaches to visualizing the same information, such as color or color intensity. For example, consider the variable ocean temperature, with values presented as different shades of blue. Darker blues might represent colder temperatures near the poles, while lighter shades might represent warmer temperatures near the tropics and equator. And rather than a two-dimensional graph with an x-axis and y-axis, imagine projecting the values of those variables onto a map of the globe. Such a histogram is very different from a bar chart.

Bar charts and many histograms depict statistical information about the relative frequency of different values for a variable as relative height of bars or points. This can reveal features of the overall statistical distribution of values of the variable. If a histogram has a single peak, this shows that one value is the most common. The most common value is called the mode. So, a data set with just one peak, in which one value in a range is the most common, is called a unimodal distribution. If a histogram has two different peaks of similar height, then it reveals a bimodal distribution, where two values in a range are the most common; the two peaks correspond to the two most common values of the variable.

A histogram for class grade percentages nicely illustrates the difference between these distributions. A common distribution of grades is unimodal: it has one peak where the most common grades occur—often somewhere in the range of B to C. Math and logic courses often have bimodal distributions instead: they have two peaks, one at the top of the grading scale and the other in the middle or lower part of the scale. See Figure 9.4 for example histograms of unimodal and bimodal distributions.

If the height of the different bars in the histogram is the same for all values, then it shows a uniform distribution where all values are equally likely. Grading distributions are rarely uniform. Fair random variables should have uniform distributions across their values over lots of trials. For example, 1,000 dice throws should have an approximately uniform distribution across the individual outcomes of one, two, three, four, five, and six. See Figure 9.5 for a histogram depicting this uniform distribution.

Another important feature of statistical distributions visible in histograms and bar charts is symmetry—that is, whether the portions to the right and left of the mode(s) are the same. Symmetric graphs can have a uniform distribution, a U-shape, or a N-shape. A flat line—uniform distribution—is symmetric without a mode. A U-shape is a symmetric bimodal distribution where large and small values are the most common, while a N-shape is a unimodal distribution with the most common values clustered around the middle, with decreasingly common outcomes as the values get higher and lower. (See Figure 9.6.) As we will see in the next chapter, this N-shaped distribution, called a bell curve or normal distribution, is especially important in inferential statistics. Note that a symmetric histogram might not be perfectly symmetric, just approximately so.

#### Correlations

Most scientific research is concerned not just with variables but also with the relationships among them. For instance, some years ago French researchers studied whether people drink more alcohol when they hang out in loud bars. They found a positive correlation between the variable decibel level in bar and the variable alcohol consumption.

If you ask whether level of marijuana consumption is different in different states in the US, you are interested in the relationship between the variable marijuana consumption and the variable state of the USA. Or, if you wonder whether being able to read at a younger age predicts salary level in adulthood, you are again asking about the correlation between the values of two variables.

Recall the definition of statistically independent in Chapter 8. When two variables are statistically independent, the value of one variable does not raise or lower the probability of the other variable taking on any given value. Variables that are not statistically independent are correlated variables: the value of one raises or lowers the probability of the other having some value. For example, the correlation found by those French researchers between loud bars and alcohol consumption means that a person going into a loud bar is more likely to have more alcoholic drinks than is a person going into a quiet bar.

When greater values for one variable are related with greater values for a second variable, these variables are said to be positively correlated. Decibel level of bar and alcohol consumption were found to be positively correlated. When greater values for one variable are related with smaller values for a second variable, these are said to be negatively correlated. Perhaps level of alcohol consumption on a given evening is negatively correlated with waking up early the following morning: the more alcohol someone drinks, the less likely that person is to wake up early.

For quantitative variables, scatterplots can provide a visual representation of whether and how the variables are correlated. A scatterplot is a graph in which the values of one variable are plotted against the values of the other variable. For example, the horizontal axis of the plot, the x-axis, may report the decibel level in different bars, and the vertical axis, the y-axis, the average number of drinks consumed in those different bars, as shown in Figure 9.7.

As one variable gets larger, the values of the other variable also tend to get larger. However, there can be exceptions—dots that vary from that general pattern. Some very quiet bars may serve a lot of drinks, and some very loud bars may serve few drinks. But this needn’t eliminate the general correlation between decibel level and alcohol consumed.

A scatterplot that shows a negative correlation between variables will have dots that tend to form a downward‐sloping line from left to right. As the values of one variable get larger, the values of the other variable tend to get smaller. Of course, there can be dots that vary from this pattern as well without interfering with the negative correlation.

What would you expect for a scatterplot of two variables that aren’t correlated? Well, there won’t be an upward sloping line, and there won’t be a downward sloping line. What you usually see are dots all over the place, with no pattern between the values of one variable and the values of the other variable.

One compact way to summarize the relationship between variables is called regression analysis. The basic idea is to find the best‐fitting line through the points on a scatterplot. Modern regression analysis was invented in the late 19th century by Sir Francis Galton. Galton was a geographer, meteorologist, tropical explorer, inventor of fingerprint identification, eugenicist, best‐selling author, and half‐cousin of Charles Darwin. He was obsessed with measurement. In 1875, Galton began to investigate heredity: why do successive generations remain alike in so many features? And how do offspring vary from their parents? One of his projects was to measure the diameter and weight of thousands of mother and daughter sweet pea seeds (see Table 9.2). He plotted his results and hand‐fitted a line to his data as best as he could.

##### Width of parent/offspring sweet pea seeds

| Seed | Mother | Daughter |
| ---- | ------ | -------- |
| #1   | 15     | 15.3     |
| #2   | 16     | 16.0     |
| #3   | 17     | 15.6     |
| #4   | 18     | 16.3     |
| #5   | 19     | 16.0     |
| #6   | 20     | 17.3     |
| #7   | 21     | 17.5     |

Source: Galton 1889: 226

Galton wanted to find the line that best fit his data. Intuitively, this is the line that runs closest to the points scattered on a plot. Galton aimed to draw a line that minimized the sum of the distances of the points on the plot from that line, while still maintaining a straight line. In this sense, it can be considered the best fit. Figure 9.8 shows the best-fitting straight line for Galton’s data.

As you might be able to guess from what we’ve already said about scatterplots, when there is a positive correlation, the line that best fits the dots on a scatterplot has an upward-sloping trajectory as it moves right, and when there’s a negative correlation, the line has a downward-sloping trajectory as it moves right. IQ and SAT scores show a positive correlation: the slope of the regression line that describes the relationship between IQ and SAT scores is from the bottom left to the top right of a scatterplot. (See section 9.4 for discussion of what we can, and can’t, conclude from that correlation.) In contrast, speed and accuracy in carrying out a task are negatively correlated: as speed increases, accuracy decreases. In this case, the slope of the regression line goes from upper left to lower right on the scatterplot.

A regression analysis also gives information about the strength of correlation: how predictable the values of one variable are based on the values of the other variable. The closer the dots are to the best-fitting line, the stronger the correlation, that is, the more linked the values of the two variables. (Notice that the slope of the line is not related to correlation strength; the slope only gives information about how the values of the variables tend to relate to each other.) A maximum strength correlation, often called a perfect correlation, will have all the dots directly on the regression analysis line. A very weak correlation will have dots that almost look uncorrelated; they fall all over the place, far from the line, but there’s just a hint of a relationship between the values of the two variables. In Figure 9.9, you can see examples of a very strong correlation and a very weak correlation that have the same relationship among variables, so identical regression analysis lines.

From his regression analysis, Galton saw that, as the size of a mother sweet pea seed increased, so did the size of its daughter sweet pea seed. However, the daughter seeds tended to be less extreme in size compared to their mother peas: they “regressed” back towards average pea‐size. Extremely large mother seeds grew into plants whose daughter seeds tended not to be as extremely large, and extremely small mother seeds grew into plants whose daughter seeds tended not to be as extremely small. Galton called this loss of extremity the regression to the mean. It can be explained as just an effect of variability: if a variable has an extreme value, then most other values that variable can have will be less extreme. So, even though mother and daughter pea sizes are positively correlated, extreme‐sized peas tend to have less extreme‐sized daughter peas (but even this is something that can vary). The same also holds true in reverse: extreme‐sized peas usually have less extreme‐sized mother peas.

Galton also determined a correlation coefficient for mother and daughter pea‐size. A correlation coefficient provides information about the direction and strength of correlation. It has two parts: a positive (+) or a negative (−) sign to indicate positive or negative correlation respectively, and a number between 0 and 1 to indicate the strength of the correlation. This is a measure of the dispersion of the points on the scatterplot. The stronger the relationship between the two variables, the closer the correlation coefficient is to 1, when the value of one variable is a perfect predictor of the value of the other variable. A value of 0 means that the points on the plot are randomly scattered and the two variables are statistically independent: the value of one gives no information about the value of the other.

##### Box 9.1 The history of statistics and eugenics

Statistics emerged as a distinctive discipline concerned with the collection, analysis, summary, and representation of information drawn from data in the 19th century, in response to the needs of industrializing and colonizing states like the UK. Researchers who made substantial contributions to the emergence of statistics included Francis Galton, Karl Pearson, Charles Spearman, and Ronald Fischer. Born and raised at a time when and in places where Anglo-Saxon White supremacism was widespread, these men held virulently racist views. Those views influenced their work in statistics on the measurement and interpretation of group differences. One of their explicit social aims was to create a society with the most desirable traits by suppressing reproduction in people deemed to be inferior. Galton coined the term eugenics (from the Greek: “well-born”) for the idea that a human population can be improved by controlling breeding. He advocated against mixed-race marriages and in favor of incentives to encourage able, upper-class White couples to have children. In the 20th century, eugenics movements in several countries implemented policies restricting human liberties and threatening human dignity, including forced birth control, marriage restrictions, racial segregation, compulsory sterilization, and even genocide. The historical development and political abuse of tools in statistics is just one example of how scientific research has been misused to justify unethical and abhorrent practices.

##### EXERCISES

9.7 Recall: Indicate the defining features of each of the following ways to depict statistical information graphically: pie chart, bar chart, histogram, and scatterplot.

9.8 Think: Divide the following list of variables into qualitative and quantitative variables. For the quantitative variables, say which are discrete and which are continuous.

- a. the height of a mountain
- b. the color of starfish
- c. the breed of a dog
- d. the mass of a planet
- e. the winner of Wimbledon

- f. the population of a city
- g. the outcome of a throw of a die
- h. the GDP (gross domestic product) of a country
- i. type of pizza
- j. the number of pizzas one person eats per week
- k. the amount of salt in the Atlantic Ocean

9.9 Apply: Label each of Figures 9.2, 9.3, 9.4, and 9.7 as a pie chart, bar chart, histogram, or scatterplot. Then, for each, write a few sentences interpreting what each chart or graph depicts about the data.

9.10 Apply: Draw simple histograms depicting each of the following:

- a. a normal distribution
- b. a unimodal distribution that is not normal
- c. a bimodal distribution
- d. a uniform distribution

Then, draw a small line on the x-axis to indicate the mode(s) of any distribution that has one or more mode. Finally, describe (in words) what feature(s) distinguish the distributions in (b), (c), and (d) from the normal distribution in (a).

9.11 Think: A great many variables seem to be distributed normally. People’s heights, examination grades, IQ scores, milk production of cows, errors in measurement, and so forth all appear to be captured by the normal (or Gaussian) distribution. And yet this doesn’t mean all other distributions of frequency are abnormal. Do some research to find out why the normal distribution is called “normal.” Report your findings in a brief paragraph, and explain how the label “normal” might be misleading.

9.12 Apply: Label each of the following as a scatterplot, regression analysis, or correlation coefficient. Then, evaluate the direction and strength of the correlation depicted in each. That is, say whether each correlation is positive or negative, and rank them from strongest to weakest.

- a. −0.99
- b. +0.28
- c.
- d.



### 9.3 CENTRAL TENDENCY AND VARIABILITY

After reading this section, you should be able to:

- Explain the difference between the central tendency and the variability of a data set
- Define three measures of central tendency and find each for a data set: mode, median and mean
- Define three measures of variability and find each for a data set: range, variance, and standard deviation

#### Measures of central tendency: mean, median, and mode

Consider a ᑎ‐shaped histogram, like pictured in Figure 9.6 b. There is one peak at the center, corresponding to the most common group of values of the variable. There are also two “tails,” the values that are less and less common the further away they are from the most common group in the middle. As we’ve discussed, the peak is the mode; this is one measure of central tendency, the most common or typical value(s) for a random variable. The distribution of occurrences across values, even the uncommon ones, is the variability (or dispersion) of the data set. Measures of variability capture the extent to which the distribution of values in a data set is stretched or squeezed.

Measures of central tendency and variability are summary measures that can compactly describe a whole data set. Let’s consider measures of central tendency first; in the next subsection, we’ll consider how to measure variability.

Imagine this situation. Your instructor has just returned the class’s first quiz. You see that your grade is 6/10; your percentage of correct answers is 60%. How should you react? Perhaps you judge that you performed poorly—a 60% is quite low, isn’t it? Yet another reaction might be to withhold judgment until you have additional information. You might want to compare scores with your classmates or inquire how the class performed as a whole or “on average.” This additional information about the distribution of scores would help you know whether you did poorly on the quiz, and if so, how poorly.

Imagine the students’ grades are as outlined in Table 9.3. Your instructor can provide you with three different answers to the question of how the class on average did on the quiz. These correspond to three different measures of the central tendency of a distribution (of grades, or anything else). These measures are the mode, median, and mean of the distribution.

The mode is the most frequent or most numerous value in the data set. As you can see from Table 9.3, the mode of the class’s scores is 7/10. Four students scored a 7, which was the score that was more common than any other score.

The mode can be informative, and for qualitative variables, it may be the only measure of central tendency that can be employed. However, even for a unimodal normal distribution, or bell curve, the mode may not reflect the central tendency of a distribution well. Notice from the list of ordered scores that a 7, although the most frequent score, is lower than half of the students’ grades. Some distributions have

An imagined data set and central tendencies for 17 student scores on the first 10-point quiz

| Student | Score | Student | Score | Central Tendency        |      |
| ------- | ----- | ------- | ----- | ----------------------- | ---- |
| #1      | 0.0   | #10     | 8.0   | mode: 7.0               |      |
| #2      | 5.0   | #11     | 8.0   | median: 7.5             |      |
| #3      | 5.0   | #12     | 8.5   | mean: 7.1               |      |
| #4      | 6.0   | #13     | 8.5   | variance: 5.0           |      |
| #5      | 7.0   | #14     | 9.0   | standard deviation: 2.2 |      |
| #6      | 7.0   | #15     | 9.0   |                         |      |
| #7      | 7.0   | #16     | 9.0   |                         |      |
| #8      | 7.0   | #17     | 10.0  |                         |      |
| #9      | 7.5   |         |       |                         |      |

more than one mode, which also limits the ability of the mode to capture the central tendency. Finally, if all values were different, then none would be more common than any other; such a distribution would have no mode at all.

The median is the very middle value in a distribution when the values are arranged from the lowest to the highest (or from highest to lowest). By “very middle,” we mean that the median value splits the distribution exactly in half: half of the values are on one side, the other half on the other side. In our example, the median will be whatever score was earned by the student who did the ninth best/worst; the reason is that there were 17 students total, and so eight scored above that ninth student while eight scored below. Student #9 earned a 7.5 on the quiz. When the distribution has an even number of values, the median is the average of the two middle values.

The median is the preferred measure of central tendency when the distribution is not symmetrical. This is because the median is not strongly affected by outliers, that is, by data values that are remarkably different from the rest like student #1, who scored a 0 on the quiz. But that strength is also a weakness, depending on the nature of the information you want to capture. You might want the central tendency measure to be different when some students bombed the quiz instead of all having scores grouped around the middle value. Further, unlike the mode, the median cannot be identified for qualitative variables, since the values these types of variables cannot be ordered from lowest to highest. (Is cappuccino lower or higher than cortado?)

The word average is also used to refer to the mean, which is the sum of all values in the data set divided by the number of outcomes. The class’s scores summed to 121.5, and so dividing that sum by 17 gives us a mean grade of 7.1 on this imagined quiz. Like the median, the mean cannot be calculated for categorical or qualitative data, as such values cannot be used in addition. Unlike the median, the mean is affected by outliers; the mean is pulled in the direction of the distribution’s longer tail. The student who scored a 0 on the quiz pulled down the mean score by nearly half a point compared to the median.

When a distribution is unimodal and perfectly symmetrical, the mode, median, and mean coincide, and they are all exactly in the middle of the distribution. Asymmetric and multimodal distributions can lead these measures of central tendency to be radically different from one another.

To calculate a population’s mean (μ) you sum all the data, xi, in your data set and divide by the total number of data, N:

$\mu=\frac{x_1 + x_2 + \ldots + x_n}{N}$

To find either the mode or the median, you should begin by ordering all the values in the data set from least to greatest. To find the mode, count how many times each value occurs; the value that occurs most often is the mode. There may be no mode, if no value appears more often than any other, or there may be two or more modes if two or more values occur the most often. For example, if your data set is {1, 3, 3, 4, 5, 5}, then 3 and 5 are the modes.

To find the median, search for the value in the very middle of the ordered data set; the middle value is the median. If there is an even number of data, and thus no single middle value, then the average of the two values in the middle is the median. For example, if your data set is {12, 14, 15, 17, 17, 19}, then 16 is the median.

#### Measures of variability: range, variance, and standard deviation

Variation is at the heart of what it is to be a random variable. And, like central tendency, variation can be measured. Measures of variation provide us with a summary of the spread of the values in a data set—that is, the degree to which they vary.

Information about variability can differentiate data sets that have the same central tendency, that is, the same mean, median, and mode. Let’s return to our simple imaginary example of quiz grades. Suppose the next quiz has the exact same mode, median, and mean as the scores shown in Table 9.3. This suggests the class did equally well on this next quiz. And, on average, they did. But this isn’t the whole story; compare the quiz grades in Table 9.4 to those in Table 9.3. What differences do you notice?

This second data set may have the same mean, median, and mode as the quiz scores from Table 9.3, but there is much less variation in scores. Visualizing the two data sets with a histogram makes it easier to spot the differences (see Figure 9.10). There is more variation in the grades on the first quiz (Table 9.3) than on the second (Table 9.4). As this illustrates, measures of central tendency don’t capture all the information about a statistical distribution; you also need measures of variability.

There are three primary measures of variability: the range, variance, and standard deviation of a distribution. The range is the difference between the smallest and largest.

An imagined data set and central tendencies for 17 student scores on the second 10-point quiz

| Student | Score | Student | Score | Central Tendency        |      |
| ------- | ----- | ------- | ----- | ----------------------- | ---- |
| #1      | 4.0   | #10     | 7.5   | mode: 7.0               |      |
| #2      | 5.0   | #11     | 8.0   | median: 7.5             |      |
| #3      | 5.0   | #12     | 8.0   | mean: 7.1               |      |
| #4      | 6.0   | #13     | 8.0   | variance: 1.9           |      |
| #5      | 7.0   | #14     | 8.5   | standard deviation: 1.4 |      |
| #6      | 7.0   | #15     | 8.5   |                         |      |
| #7      | 7.0   | #16     | 8.5   |                         |      |
| #8      | 7.0   | #17     | 9.0   |                         |      |
| #9      | 7.5   |         |       |                         |      |

To find the range, you can just subtract the smallest value from the largest value. For the first quiz, the range was 10, since the lowest score was 0 and the highest score was 10; for the second quiz, the range was 5, since the lowest score was 4 and the highest a 9.

Range does not take outliers into account very well, since it doesn’t specify anything about the distribution of scores within the range. In other words, range won’t tell you whether the distribution’s tails are skinny or thick—the “spread” of the data. This can be done with a measure of the distance of values from the mean, such as variance or standard deviation. Both of these measures summarize the spread, or how close the various values are to the mean.

Population variance (σ²) is the average of the squared differences of values from the mean:

σ² = ∑(value − mean)² / N

The sigma (∑) indicates that you should sum all instances, and N is the number of values in the data set. Notice that calculating variance requires knowing the mean of a data set. After calculating the mean, the first step to finding the variance is to find the difference of each value from that mean; this is the distance between the mean and each value in the data set. Finding this difference will show whether the values tend to vary a lot or only a little from the mean. Next, each difference is squared. (Otherwise the differences on either side of the mean would cancel each other out, since the difference for values greater than the mean is positive and the difference for values less than the mean is negative.) Finally, find the average of those squared differences by adding them together (∑) and then dividing by the number of values (n).

##### Number of students with each test score

| 4.5  | 4    |
| ---- | ---- |
| 3.5  | 3    |
| 2.5  | 2    |
| 1.5  | 1    |
| 0.5  | 0    |

0  1  2  3  4  5  6  7  7.5 8 8.5 9  10

0  1  2  3  4  5  6  7  7.5 8 8.5 9  10

Let’s find the variance for our population of scores in the first and second quizzes. For both, the mean is 7.1 (rounded to one decimal point).

For the first quiz, the population variance would look like this:

(0.0 − 7.1)2 + (5.0 − 7.1)2 + (5.0 − 7.1)2 + (6.0 − 7.1)2 + (7.0 − 7.1)2 + (7.0 − 7.1)2 + (7.0 − 7.1)2 + (7.5 − 7.1)2 + (8.0 − 7.1)2 + (8.0 − 7.1)2 + (8.5 − 7.1)2 + (8.5 − 7.1)2 + (9.0 − 7.1)2 + (9.0 − 7.1)2 + (9.0 − 7.1)2 + (10.0 − 7.1)2 ] / 17 = 5.0

For the second quiz:

(4.0 − 7.1)2 + (5.0 − 7.1)2 + (5.0 − 7.1)2 + (6.0 − 7.1)2 + (7.0 − 7.1)2 + (7.0 − 7.1)2 + (7.0 − 7.1)2 + (7.5 − 7.1)2 + (7.5 − 7.1)2 + (8.0 − 7.1)2 + (8.0 − 7.1)2 + (8.0 − 7.1)2 + (8.5 − 7.1)2 + (8.5 − 7.1)2 + (8.5 − 7.1)2 + (9.0 − 7.1)2 ] / 17 = 1.9

Comparing the variances for the two quizzes makes clear that scores on the first quiz had more variation than on the second quiz.

The final measure of variation to discuss is the standard deviation (σ), which is calculated directly from the variance. For a population, the standard deviation is just the square root of the population variance:

σ = √[∑(value − mean)² / N]

Returning to our quiz example, the standard deviation for the first quiz is 2.2 (the square root of 5.0). For the second quiz, it is 1.4 (the square root of 1.9). The standard deviation provides us with a sort of “yardstick” for measuring variation. It is a number against which you can assess individual values or groups of values to see how far they are from the mean, relative to total variation in the data set.

If the histogram describing our data set is bell-shaped (unimodal and roughly symmetric), then around 68% of the values fall within one standard deviation of the mean.

Standard deviation in a normal distribution; the values within one standard deviation of the mean account for 68.27% of the values in the data set, while those within two standard deviations account for 95.45%, and those within three standard deviations account for 99.73% and around 95% of the values fall within two standard deviations of the mean—that is, fall within the distance that’s twice as long as the standard deviation value. And, virtually all of the values lie within three standard deviations of the mean. Look at Figure 9.11: this plot shows the locations of one, two, and three standard deviations for a probability distribution associated with a bell‐shaped histogram. The standard deviation distances will, of course, change depending on the “spread” of the data. The standard deviation value reflects this by being a relatively large number (lots of spread) or a relatively small number (little spread).

Mean and standard deviation are the most commonly reported summary statistics for a data set. Together, the mean and standard deviation capture central tendency and variability around that central tendency in a way that is informative and—as we will see in the next chapter—central to statistical inference.

You can calculate mean and standard deviation in four steps. First, find the mean. Second, for each value in the data set, subtract it from the mean and then square each result. Third, calculate the average of the resulting values (that is, sum the results and divide by the number of values) to find the population variance. And fourth, find the square root of the population variance, which is the standard deviation.

##### EXERCISES

9.13 Recall: List three measures of central tendency and three measures of variability. What roles do the concepts of central tendency and variability play in statistics?

9.14 Apply: Consider the following three data sets:

A = {4, 10, 11, 7, 15}

B = {10, 10, 10, 10, 10}

C = {1, 1, 10, 19, 22}

1. Without doing any calculations, guess which data set has the largest standard deviation.
2. Calculate the mean, mode, and median of each data set.
3. Calculate the range, variance, and standard deviation of each data set.

9.15 Think: In Israel in 2022, the average person earns about 12,000 Israeli shekels (more than 3,000 US dollars), their height is about 1.75 meters, and weight 85 kilograms. Do you think this “average person” really exists? Why or why not?

Now, consider that medical drugs work a certain way on average. For example, for adults, drug dosage is set for all adults by considering average weight. How should we understand this feature of medical drugs? What are the advantages and disadvantages of medical drugs working certain ways on average?

9.16 Think: Suppose the waiting time to be seated at a restaurant is an average of 20 minutes and has a standard deviation of 2 minutes. What can you conclude about when people are seated?

9.17 Apply: On one x- and y-axis, draw three histograms approximately depicting normal distributions for each of the following pairs of means and standard deviations. Label each with (a), (b), or (c), depending on which pair it depicts.

- a. mean = 4, standard deviation = 2
- b. mean = 8, standard deviation = 0.5
- c. mean = 8, standard deviation = 4

9.18 Apply: 

1. Reconsider Table 9.2, which provides a simple data set used by Galton in his studies of heredity, and represent the data set with either a pie chart, bar chart, or histogram. (Choose carefully!) You might want to review what characteristics of a variable’s values makes it well suited for each of these types of charts.
2. Find the mean, median, and mode of this data set. Next, calculate its range, variance, and standard deviation.

### 9.4 SLOPPINESS, HYPE, AND MISUSES OF STATISTICS

- List three ways in which visual statistical representations, correlations, and numerical statistical representation can go wrong.
- Describe the difference between absolute and relative risk.
- Analyze visual and mathematical representations of statistical information for accuracy and potential problems

#### Misleading presentation of statistical information

So far in this chapter, we’ve seen how statistical data can be meaningfully represented visually in charts, tables, and graphs and mathematically with measures of central tendency and variability. These visual and mathematical techniques of representing trends in the values of variables and correlations among them are powerful. With these techniques, complex data can be depicted in ways that are simpler and easier to understand. Unfortunately, these same features of visual and mathematical representation of statistical data also make these techniques susceptible to misuse—unintentionally and even intentionally, when they are used to exaggerate or mislead.

Imagine you just read on social media that a vegan diet increases the risk of a rare skin disease by 100%. Learning this scary news is based on a study by a group of nutritionists; so you and many other vegans in the country decide to change your diet. Puzzled by your decision, your friend goes and actually reads the original study. The study says that, out of 20,000 omnivores, the researchers found that one person developed the rare skin disease, and, out of 20,000 vegans, two developed it. What the study found is that the absolute increase in the risk of developing the disease is only 1 in 20,000. Nothing to worry about—you may stick to your vegan diet.

The attention-grabbing, and scarier, 100% figure is the relative increase in risk found in that study. You may have thought you understand what 100% means, but you were misled. Although the absolute and relative risk in that study quantifies the same effect, you may not be aware of the difference between the two. An absolute risk is the number of individuals experiencing some condition—say, a rare disease—in relation to the relevant population—say, the general population of vegans. A relative risk is a comparison between the incidence of a condition in two groups—say, the groups of vegans and omnivores—and is generally expressed as a ratio or percentage. If the media had reported the findings of that study in terms of absolute risk, nobody would have paid attention.

Like the media, scientists also compete for attention. After all, their research is often dependent on grants, which can prize impact and showy findings rather than modest studies that only contribute a tiny bit to the existing knowledge. This type of pressure can lead the popular media and some scientists to misleadingly hype or misrepresent scientific findings. Statistical information can also be misleading simply due to sloppiness. Sometimes media outlets or reporters misunderstand the implications of scientific research they are reporting on.

All of this can make statistical information opaque and hard to understand for the general public. For example, bar charts (introduced in section 9.2) often feature in scientific work, as well as newspapers, magazines, and social media. They afford an automatic feeling of understanding. But, like numbers, graphs and diagrams can easily be exploited for communicating statistically information misleadingly. In an article on how statistical information can be made more transparent, Kurz‐Milcke and collaborators illustrate this point with two bar graphs presenting the same type of statistical information from a study on heart conditions.

The bar chart on the left compares the incidence of two conditions, stroke and major bleeding, in patients with a heart condition who took either aspirin or warfarin and patients who did not have any treatment. You can see clear differences between the heights of the three bars, showing that aspirin is better than no treatment at all, but warfarin is better than aspirin at preventing these conditions. But this representation invites the same confusion between absolute and relative risk reduction we described earlier. The problem is apparent from the bar chart on the right, where the same statistical information is presented in terms of a reference group of 100 people with the heart condition. With the addition of this reference class, we get a sense for absolute risk: it’s true that warfarin decreases risk of stroke compared to aspirin or no treatment at all, but that reduction is risk is merely from 3%–4% risk (no treatment) or 2% risk (aspirin) to 1% risk. This may still be valuable risk reduction, but it is a much more modest improvement than it seemed from the chart on the left.

This is just one example of how communicating statistical information transparently can be challenging and can offer perverse opportunities to mislead the scientific and public audiences.

#### Thinking critically about correlations

In section 9.2, we noted there is a correlation between IQ and SAT scores. But there is also a strong correlation between a child’s socioeconomic status and their IQ scores, as well as between their socioeconomic status and their SAT scores. So, we can ask whether IQ is responsible for SAT scores at all, or if socioeconomic status is just responsible for both.

There’s a familiar adage: correlation is not causation. If you find a correlation between higher IQ scores and higher SAT scores, for example, you cannot automatically conclude that higher IQ causes higher SAT scores. That’s a sloppy and misleadingly hyped way of presenting your finding. There are other possible reasons for this correlation.

It’s true that we can learn a lot from correlations. We’ll focus on causal reasoning in Chapter 11, but it’s already important to clarify that, although correlation differs from causation and does not guarantee causation, the presence of a genuine correlation among a set of variables is a defeasible clue to causal relationships. Having an accurate picture of correlations among variables helps scientists understand interrelationships in complex phenomena—like education and scholastic performance. An accurate pattern of correlations can also guide the design of experimental studies aimed at finding out about the causal structure of those phenomena. Yet, despite these and other uses, “correlation is not causation” is a familiar saying because it’s all too common for scientific research and science reports in the popular media to assume causation from mere correlation. Given this tendency, you should think critically about what correlation demonstrates, especially correlations uncovered with non-experimental studies involving a great many variables.

Here’s an example. Researchers randomly picked 50 ingredients from recipes in a cookbook and, for each, looked for published nutrition science research concluding that the ingredient affects one’s risk of cancer. They found that 40 of the 50 ingredients—including tea, tomatoes, carrot, parsley, nuts, and bread—were claimed to have an impact on cancer risk. But most of the claims were based on single, non-experimental studies reporting implausibly large correlations. Some of these correlations might have been spurious too, since the larger the number of variables involved in a study, the higher the chance of finding a correlation between some ingredient and some health outcome. The sheer volume of research on foods and cancer risk—and popular media reports of this research—can lead to outsized attention to specific nutrients compared to other well-established and significant influences on cancer risk, such as alcohol and tobacco use.

##### Box 9.2 Panic headlines

Behavioral economist Emily Oster quickly gained a following for her no-nonsense analysis of scientific research bearing on everyday decisions, especially decisions related to pregnancy and parenthood. She has an electronic newsletter, and some of her newsletters discuss what she calls “panic headlines.” Oster is referring to the tendency of popular media to report scientific findings—especially those bearing on health—in dramatic or even hyperbolic ways. One such headline from early 2023 was, “Wine before pregnancy changes baby’s face.” Panic headlines make us more likely to read the article, but they can also provoke misplaced fear and overstate scientific findings. Fortunately, some of what we’ve learned in this book can be used to help put panic headlines in context. Here are some questions to ask about any popular reporting of scientific findings:

1. Do the scientific findings reported fit with the headline? Many headlines are misleading, overstate conclusions, or focus on one small detail instead of the big picture.
2. Is the report based on one research study? If so, what is the consensus in the field? Are there any meta-analyses of studies you can consult?
3. Can you tell whether the research attempted to control variables? If not, is there reason to suspect the correlation is spurious or influenced by confounding variables?
4. Is there any indication of the strength of the correlation or effect size? Identifying a correlation or an effect doesn’t guarantee the relationship is strong.

Let’s now consider the correlation between IQ, SAT, and socioeconomic status. IQ scores and SAT scores are highly correlated, and there’s a straightforward reason for why: both are standardized tests designed to measure similar features of intellectual ability and aptitude. Focusing only on that relationship might lead to an assumption that IQ measures innate ability, and in turn causes SAT performance. And indeed, this view dominated, at least in the US, in the late 20th century. But this view of IQ is too simple. IQ scores and SAT scores are both influenced by a variety of life circumstances. Schooling—from preschool through adulthood—influences intelligence as measured by IQ scores. Experiences associated with poverty, beyond access to quality schooling, also influence intelligence as measured by IQ scores, including healthcare, social services, family resources, and levels of stress.

#### Can the numbers lie?

We’ve seen that visual presentation of statistical information can be misleading, and correlations misused or misinterpreted. You might think that numerical statistics themselves, including measures of central tendency and variability of a data set, cannot lie, since these are straightforward, simple-to-calculate summaries of a data set. Alas, this is not always true.

One issue with statistical information to keep an eye out for is statistical data that look too neat. As we’ve seen, the values of random variables are inherently noisy. Statistical patterns can emerge in aggregate data, but variability and exceptions still arise. Statistical information appearing too neat can mean that the data are presented misleadingly, or even that the data are partly or entirely fake. We introduced the idea of data cleansing—identifying and correcting errors in a data set by deciding which data are questionable and should be eliminated—in Chapter 5’s discussion of data models. But that process can go too far, resulting in choices about data inclusion that artificially lead the data to fit with expectations. Data from different samples or collected at different times should vary somewhat, in terms of their central tendencies like the mean and also in terms of their variability like range. The absence of such variability is a reason to be suspicious of the data.

In 2010, economists Carmen Reinhart and Kenneth Rogoff published research providing telling evidence in favor of austerity. Using data from many countries, the two economists focused on the relationship between the rate of growth of a country’s economy, the public debt of the country and its gross domestic product, or GDP. (GDP is a measure of the added economic value created with new goods and services a country can produce during a certain period.) Their main conclusion was that it’s bad for economic growth when the debt-to-GDP ratio of a country is above 90%. The paper received wide coverage in the media and played a role in many countries’ policies of austerity measures, that is, spending cuts and tax increases to reduce deficits in the face of recession. But it later emerged that the results of that paper were based on a mistake in countries’ reported debt level in the Microsoft Excel spreadsheet the two economists had used for their statistical analyses. Though the paper had concluded that average growth above the 90% ratio was negative, the average was actually positive! This entirely undermined the researchers’ conclusion that had been so influential over global economic policies.

Summaries of a data set are only as good as the data set and calculations performed on them. We have seen that data are affected by measurement error and other sources of inaccuracies, and both data and analysis are subject to human error. Sometimes, as in the case of Reinhart and Rogoff’s research, they are simple, honest mistakes; other times they are due to bias or incompetence. The psychologist Michèle Nuijten worked with a group of statisticians and methodologists to analyze how common numerical errors are in scientific research, focusing on more than 30,000 papers published in major psychology journals over three decades. These researchers used a computer algorithm to flag potential mistakes, and they found that about half of the papers in their sample had at least one inconsistency. Further, the inconsistencies tended to make the results of a study more likely to support the study’s hypothesis; so at least some of the errors seemed to have been consciously or unconsciously incorporated to increase support for the research findings.

The upshot of this section is that, even as statistical information is highly valuable in presenting information and assessing claims, its power is also a liability. Statistical information, presented visually or numerically, appears decisive but can, through sloppiness or intentional misuse, invite misinterpretation and mistaken conclusions. The appearance of statistical information isn’t enough to guarantee trustworthiness: in scientific research and popular reporting alike, the use and presentation of statistical information needs to be carefully interpreted and assessed for its accuracy.

##### EXERCISES

9.19 Recall: List three ways in which visual statistical representations, correlations, and numerical statistical representation can go wrong, giving an example of each.

9.20 Apply: Characterize the difference between absolute and relative risk. Then, describe a new example of relative and absolute risk in health research. (An internet search is one way to find an example, but be discerning about the source and content you choose!)

9.21 Recall: Describe how information about correlations can be misleading, using the example of IQ, SAT scores, and social factors.

9.22 Think: Briefly summarize the discussion of correlation and causation in this section. Then, describe how this relates to confounding variables (see Chapter 3).

9.23 Apply: Imagine a company advertises the toothpaste they produce by declaring, “More than 80% of dentists recommend our toothpaste.” (a) Describe the most reasonable interpretation of this claim, explaining the type of survey evidence that would support it. (b) Suppose the actual survey given to dentists by the company asked them to indicate several toothpastes and brands they might recommend instead of only one. In light of this additional piece of information, explain how the prior advertisement may be misleading.

9.24 Apply: You can find many examples of misleading mathematical and visual representations of statistical information in the news and social media. Consider this example from September 2021, where a news program in the US used the chart pictured here to show how the number of Americans claiming to be Christians is decreasing. (a) What does the graph visually suggest? (b) Why is this graph misleading? (c) How should it be redrawn not to mislead the public?

##### FURTHER READINGS

For a comprehensive history of statistics and statisticians, see Stigler, S. M. (2002). *Statistics on the table: The history of statistical concepts and methods*. Harvard University Press.  .   .

For more on the nature of averages and the normal distribution, see Stigler, S. M. (1997). Regression towards the mean, historically considered. *Statistical Methods in Medical Research, 6(2), 103–114; Lyon, A. (2014). Why are normal distributions normal? The British Journal for the Philosophy of Science, 65(3), 621–649; and Røislien, J., & Frøslie, K. F. (2019). Jane and John do not exist. Tidsskrift for Den norske legeforening*. https://doi.org/10.4045/tidsskr.18.0824

For a focused discussion of how to understand correlations and what can be learned from them, see Anjum, R. L., & Mumford, S. (2018). What’s in a correlation? In R. L. Anjum & S. Mumford (Eds.), *Causation in science and the methods of scientific discovery* (ch. 4, pp. 29–36). Oxford University Press. https://doi.org/10.1093/oso/9780198733669.003.0004

For a comprehensive survey of sloppiness, bias, hype, fraud in statistical—and more generally, scientific—practices, see Ritchie, S. (2020). *Science fictions: Exposing fraud, bias, negligence and hype in science*. Random House.