## CHAPTER 5 Scientific modeling

###  5.1 THE SAN FRANCISCO BAY AND THE VALUE OF SCIENTIFIC MODELS

After reading this section, you should be able to:

- Describe how the Bay model was developed and how it was used to evaluate the Reber plan
- Define scientific model and target system, and indicate why models need to be partly similar to and partly different from their target systems
- Analyze the similarities and differences between the Bay model and the San Francisco Bay and how each is valuable or problematic

#### The Bay model

In an unassuming warehouse in northern California, there lies an enormous model of the San Francisco Bay and the surrounding Sacramento–San Joaquin River Delta. The San Francisco Bay area is a large body of ocean water surrounded by a large urban population living in diverse geological terrains and climates. The delta surrounding the bay is an area the size of the US state of Rhode Island, stretching from the Pacific Ocean almost halfway across the width of California. The Bay model is huge: it’s over 1.5 acres in size (more than 6,000 square meters) and is made of 286 five‐ton concrete slabs pieced together like a jigsaw puzzle. Still, as large as it is, the Bay model is 1,000 times smaller than the actual San Francisco Bay.

The Bay model is a hydraulic model. Pumps move hundreds of thousands of gallons of water (1 gallon is 3.785 liters) to mimic the tides and currents of the real bay. This procedure works because the model is three‐dimensional and proportional; the different parts of the bay and river delta in the model are the right amount lower than sea level, and the surrounding land is the right amount above sea level. The Bay model also includes other features that affect water flow, like rivers, canals in the delta, wharfs, bridges, and breakwaters.

The Bay model is not just a toy model made for tourists. Instead, it’s a scientific model. Scientific models are constructed to represent phenomena of interest and investigated to learn about those phenomena. This particular model is a terrific tool for

DOI: 10.4324/9781003300007-6
---

# Scientific modeling

 

  

View of the San Francisco Bay model, looking toward the Golden Gate Bridge and Pacific Ocean

# FIGURE 5.1

learning about the San Francisco Bay and how human activities can affect it. Teachers, students, and scientists use the Bay model to study geography, ecology, human and natural history, and hydrodynamics. It has been used to help answer questions about how dredging new shipping channels would affect the delta, about how mining during the California Gold Rush changed the rivers, and about what would happen if the system of dikes and levees in the delta failed.

A look at how and why the Bay model was first constructed will help us start to get a sense for the roles that models play in science. John Reber moved from Ohio to California in 1907 and set up as an amateur playwright, dramatist, and theatrical producer. Because of his work, he enjoyed social connections with numerous businessmen and politicians. In the 1940s, Reber became dismayed that the transcontinental railroad ended in Oakland rather than San Francisco, and he came to believe that the extensive bay between San Francisco and the mainland interfered with industry by isolating San Francisco from the rest of California and the US. He came to believe that this large body of water is a “geographic mistake” needing to be corrected.

Reber’s career was in entertainment; he had no expertise in science or engineering. Nonetheless, he intrepidly proposed a grand plan to renovate, and then exploit, natural features of the bay that he thought would enable more efficient use of it. Reber suggested filling some parts of the bay to create additional land for things like

# Scientific modeling

 

  

# The Reber plan: shaded areas were parts of the bay Reber proposed filling to create land

Figure 5.2: The Reber plan

This would also establish two freshwater lakes supplied by the rivers that empty into the bay. As freshwater has always been a limited resource in the San Francisco Bay area, it could be valuable to repurpose the bay for potable drinking water and irrigation.

Reber’s plan was taken seriously by politicians and capitalists, and the US Army Corps of Engineers decided to test it out. An immediate problem, though, was that the Corps couldn’t effectively do so in the actual bay without first implementing it, and, wisely, the Corps wasn’t prepared to radically alter the bay and surrounding river delta before knowing what the results would be. They recognized that such changes might have unintended negative consequences for the local water supply, wildlife, vegetation, agriculture, and human population. What to do? How could they consider the effects of the plan without going ahead and carrying it out?

This highlights one way in which scientific models can be useful. When performing an intervention on a system of interest isn’t possible, practical, or desirable, a model of the system can be used instead. The Army Corps of Engineers built a hydraulic model designed to be like the San Francisco Bay in some important respects. Once the model was sufficiently similar to the real San Francisco Bay, predictions about the bay could be made based on what was observed in the Bay model. The model could then be manipulated to determine what would happen in the real bay if the Reber plan were implemented. The scientists added scale models of Reber’s proposed dams to the Bay model to create the lakes and landmasses Reber had in mind, and then they sat back to see what would happen.

As it turned out, when the Reber plan was implemented in the Bay model, its unintended consequences were disastrous. Rather than lakes, the dams created stagnant pools of poor‐quality water that couldn’t support ecosystems or be used for drinking or irrigation. Altering the dam configuration in the model to solve that problem just created another problem: fast currents that again destroyed ecosystems and made travel in the bay significantly more dangerous. When the Corps reported these findings, the Reber plan was abandoned.

### Models and targets

The real‐world system or phenomenon that scientists want to study using a model is called a target system, or just a target. Because scientists investigate models to draw conclusions about target systems, a model needs to be like the target system in some ways; that is, it should be similar to, or resemble, the target. And not just any similarity will do. Scientific models need to be similar to their targets in relevant ways for what is being studied. This is why the Bay model replicated tides and currents and other important features to water flow in the San Francisco Bay. If the model were being used to study traffic flow across the bridges, different features would need to be similar.

The relevant similarities between a model and target system are what makes it possible to gain knowledge about a target system from studying a model. Relevant similarity can be achieved in different ways. The Bay model uses real water and simulated tides to mimic water flow, but depth and water resistance of the model had to be adjusted before the water flow was right. Different adjustments could have been made, so long as they also mimicked the real water flow. Or, the model could have been made a different size or out of different materials.

The differences between a model and target system are just as important as the similarities. The Bay model’s different spatial and temporal scales are two features that made it useful for learning about the real San Francisco Bay and delta. The model is much smaller than the real bay, with much faster tidal cycles. This allowed scientists to observe what would happen with a spatially distributed, long‐lasting sequence of events in a short time and without having to leave the model’s warehouse. Had the Corps tried to build a model exactly like the San Francisco Bay, it would have been too large to put anywhere, and it would have changed so slowly, they would have had to wait years to find out about the consequences of the Reber plan.

Some features of the bay either didn’t matter or would have been too difficult to accurately incorporate in the model. The Bay model ignores people and buildings, since these are unimportant for water flow. And being inside a big warehouse means the model isn’t exposed to changing weather like the real bay is. The model also doesn’t incorporate the oceanic wind currents that affect the bay; it’s tricky to see how those could be replicated and whether the outcome of doing so would be worth the effort.

The scientists who built the Bay model thus had to decide which features of the model should be similar to the real bay and which could, or should, be different. They also had to decide what to do about changing features of the San Francisco Bay, like whether the model should be like the actual bay during dry seasons, wet seasons, or some combination of these. These decisions were all important to constructing a useful model that could provide reliable information about how the bay would change if the Reber plan were implemented. As it turned out, the model they developed was sufficiently similar to the real bay not only to serve this purpose but for it to be put to other uses as well. For example, the Bay model was also used to study how a later plan of deepening water channels would affect water quality.

Some models are similar to their targets by exemplifying them. An exemplar is a model that is one of the target systems it is used to represent. Researchers can use exemplars to represent a broader class of targets that includes the exemplar, drawing conclusions about the whole class of targets by investigating just the exemplar. For example, the fruit fly (Drosophila melanogaster) is a common model organism in genetics and developmental biology. Biologists have used the fruit fly to learn how genes influence physical traits and how embryos develop from single cells to mature organisms.

The fruit fly’s features make it a good model for many purposes. Fruit flies are small and reproduce quickly, and large populations are easily maintained in labs. Their genome is very simple, with only four chromosomes, and scientists have it entirely mapped; so they can make precise interventions in their genes. As an exemplar, the fruit fly is used as a model organism to learn about genes in general, to show how development works in all insects, and for other applications. But, for some investigations, the differences from other organisms that make fruit flies particularly convenient models can also hinder their usefulness, like when genomic complexity is important.

##### EXERCISES

5.1 Recall: Define the terms model and target system, and indicate the model and target system(s) in one example of scientific modeling.

5.2 Apply: For both the Bay model and the fruit fly as a model organism, say what the model is, what the target system is, how the model is related to the target system, and what the model is useful for.

5.3 Recall: Describe the Bay model of the San Francisco Bay, explaining the purpose for which it was built and what one could learn about the real world from this model. Describe three similarities between the Bay model and its target and three differences, explaining the purpose of each.

5.4 Think: Identify two reasons why researchers often use simpler model organisms like the fruit fly to study human biology.

5.5 Apply: List three examples of scientific models not introduced in this section. For each, indicate what the model is used to investigate.

### 5.2 THE MODELING PROCESS

After reading this section, you should be able to:

- Compare and contrast the features and uses of the Lotka-Volterra model with the Bay model
- List three main steps involved in modeling and describe what happens in each
- Define variable, parameter, assumption, idealization, and giving an example of each from the Lotka-Volterra model

#### Specifying target systems

Scientific modeling is a way of gaining knowledge about a target system by investigating a model. The modeling process must include some initial analysis of the target system. Scientists need to decide what they are trying to learn about in order to construct a useful model. Do they want to predict the effects of proposed changes to the San Francisco Bay? Examine the genetic influences on some trait? Or, say, learn more about how number of predators influences a population of animals? All of these projects require models with different features.

An archer cannot accurately hit a target with their arrow if they don’t know where the target is or what it looks like. Without some knowledge about the target system, scientists can’t evaluate whether their models are similar enough, and in the right ways, to usefully study the target. So, at the beginning of the modeling process, scientists need to have a sense for what a model should be about and what they want to learn from the model. This can be preliminary and partial, just enough to get the process going.

For the Bay model, the task was to evaluate the feasibility and consequences of the Reber plan for damming up the bay. This was a starting point, even though scientists didn’t know exactly what would matter—currents, salinity of the water, and so on—or what they’d find—plentiful freshwater, excessive evaporation, or something else.

Once the goal of a model is settled by deciding what is to be learned about the target system, scientists also need to know something about which features of the target system are important and what those features are like. This is so they can construct a model that is similar to the target in the proper ways to be useful. When planning the Bay model, scientists figured that the tides and currents might be important to the Reber plan’s effects. To calibrate the model to have the same tides and currents as the real San Francisco Bay, researchers needed extensive data about these features of the real bay. Eighty people took measurements at different locations throughout the 1,424 square kilometer (550 square mile) bay every 30 minutes through a full tidal cycle of 48 hours. They recorded tide velocity and direction, changes in the water’s salinity (salt content), and the concentration of sediment. These and other data were needed in order to decide what features a model of the bay should have.

#### Constructing the model

After specifying the target system, researchers construct the model. This stage of modeling involves choices regarding how a model is designed to be similar to its target, what its other features are like, and to how many different circumstances or different targets a model should apply.

For the Bay model, researchers elected to construct a physical replica of the target. The San Francisco Bay is a complex system, and one advantage of a physical model is that the scientists didn’t need to understand how changes occur in the bay to predict those changes. Instead, their approach was to make the replica as similar as possible to the bay in all the ways they thought might matter, and then sit back and see what happened. Still, the model required extensive calibration—comparison with the real bay followed by adjustment—before it was accurate enough to make trustworthy predictions. The engineers used their extensive measurements of the bay’s tides and currents to ensure patterns of water flow in the Bay model were similar to those of the real bay. They had to tinker with the scales used for depth and width of the bay and the water resistance of the surface in order to get the proper water flow.

Other modeling approaches that don’t involve constructing a physical replica offer different advantages and challenges. For example, the Lotka‐Volterra model is an influential model in ecology developed independently by Alfred Lotka and Vito Volterra in the 1920s. Unlike the Bay model, the Lotka‐Volterra model does not lie in any warehouse. It’s a simple, abstract mathematical model. The Lotka‐Volterra model uses mathematical equations to represent the interactions of predators and their prey, like foxes and hares, lions and wildebeest, polar bears and seals, and so on. Here are the equations:

d x /d t = αx − βxy

d y /d t = δxy − γy

One variable, x, stands for the number of prey animals (for example, seals). Another variable, y, stands for the number of predator animals (in this example, polar bears). With these equations, biologists can calculate how predator and prey populations change over time (represented in the model as the derivatives d x /d t and d y /d t) from the combination of those population numbers and a few other parameters. A parameter is a quantity whose value can change in different applications of a mathematical equation but that only has a single value in any one application of the equation. In this equation, α, β, δ, and γ are parameters. These help the model account for, respectively, the prey population’s rate of growth without predation, the rate at which prey encounter predators, the predator population’s rate of growth, and the loss of predators by either death or emigration.

The Lotka‐Volterra model represents predator‐prey interactions, but there’s no straightforward way in which these equations are similar to animals eating other animals. Instead, the similarity is between the numbers that solve these equations for certain values of the variables and parameters and how predator and prey populations change in size over time. Recall that different choices can be made in how to achieve similarity with the target system; for models like this one, the similarity is simply in mathematical description. Mathematical models like the Lotka‐Volterra model require a firmer grasp of what about the target system is important and how these features interact than physical replicas do.

Variables and parameters are explicit parts of the Lotka‐Volterra model. What doesn’t appear are the model’s assumptions, but these are no less important. In this context, an assumption is a specification that a target system must satisfy for a given model to be similar to it in the expected way. In the Lotka‐Volterra model, numerous assumptions must be satisfied for the numbers solving the equations to indicate the actual change in predator and prey population sizes. For instance, the model assumes that prey populations will expand if there are no predators and that predator populations will starve without prey. Both assumptions are plausible. The model also assumes that prey populations can always find food, that predators are always hungry, and that predators and prey are moving randomly through a homogenous environment. These three assumptions are idealizations. An idealization is an assumption made without regard for whether it is true, often with full knowledge that it is false. These idealizations enable scientists to focus on the essentials of predator-prey interactions in general, without getting lost in complicating details of specific populations’ circumstances. Idealizations are discussed in more depth later in the chapter.

Because models can be similar to target systems in different ways, a single target is sometimes represented by multiple models. This can be useful when the real-world phenomenon is so complex that no single model can provide scientists with all they want to know about it. Weather patterns are a good example. Meteorological models normally represent only some of the factors needed to generate reliable predictions. Some may invoke humidity, temperature, and dew point to describe and predict certain basic weather patterns like precipitation. Other models may invoke other parameters, such as central pressure deficit and wind speed and direction, to better predict particular phenomena, like hurricanes. Sometimes meteorologists aim to make more reliable predictions by carefully cobbling together the results of different models of a given weather system.

It’s also possible for a single model to have more than one target system. A model might be designed to represent a type of event that recurs often. The Lotka-Volterra model is like that; it is designed to capture something important about seal and polar bear populations, wildebeest and lion populations, and many more prey and predator populations. And the same meteorological models can be used to study different hurricanes, as well as typhoons and cyclones. In contrast, the Bay model is designed to specifically represent the San Francisco Bay and surrounding delta. A new model would need to be designed to represent any other bay.

#### Analyzing the model

Once a target has been specified and a model constructed with that specification in mind, the model must be analyzed to learn about the target. Models that involve equations, like the Lotka-Volterra model, can be mathematically analyzed by inputting values for parameters and variables representing specific information about a target population. Analysis may also involve manipulating a model to see the effects of changes. This kind of physical manipulation was performed on the Bay model to test the Reber plan. For model organisms like fruit flies, scientists may alter a gene and see how the offspring change to explore the gene’s effects.

Such manipulations produce data that, if all goes well, can be used to learn about the target. This is a central purpose of analyzing a model: to draw conclusions about the target system(s). The Bay model revealed that freshwater lakes couldn’t be maintained in the San Francisco Bay, as the Reber plan called for, and that the planned dams would have disastrous unintended consequences to the local environment. It was thereby concluded that the Reber plan shouldn’t be implemented in the real San Francisco Bay. The Lotka-Volterra model, in turn, reveals that predator and prey population numbers are tightly linked. More prey leads to an increase in predators, and more predators leads in turn to a decrease in prey. This results in a cyclical relationship between predator and prey population sizes, where the population sizes go up and down together.

Visual representation of solutions to the Lotka-Volterra equations; these are values of x and y that solve the equations, which predict how prey and predator populations will covary.

The main purpose of analyzing a model is to learn about the target system(s). But another important purpose is to use existing data to assess and improve the representation of target systems. Because specifying the target and constructing the model can involve some guesswork, and because models differ from their target systems, researchers may not trust that what happens in the model will happen exactly as it does in the target. Analyzing the model’s behavior and comparing it with the target’s behavior can be used to assess the model’s success.

An example of this use of model analysis is the process of calibrating water flow in the Bay model to match water flow in the real San Francisco Bay. Engineers used their extensive measurements of tides and currents in the bay to adjust the model’s depth, width, and water resistance until its water flow matched. Ultimately, they made the model bay much deeper proportionally than the real bay, which helped. But this resulted in water moving too quickly in shallow parts of the model. Researchers compensated for this by adding 250,000 copper strips to the bay floor in the model to increase water resistance. They chose how many copper strips to add to any given place by comparing the model’s water flow with that of the real bay.

Different models with the same target are sometimes analyzed together to see whether and to what extent they have the same results. This technique is called robustness analysis. It can help determine whether models are accurate of the target when direct comparison with the target isn’t possible, like when the target is highly complex like the climate or a country’s economy. If multiple meteorological models with different variables, parameters, and assumptions all predict an upcoming increase of temperature in a region, this prediction is robust and seems to have more evidence backing it. Similar predictions from different models also can help scientists identify common features of the models, which might relate to stable relationships involved in the complex phenomenon under investigation. In this way, climatologists and other scientists studying complex systems can learn whether and to what degree the predictions of a model should be taken seriously.

##### EXERCISES

5.6 Recall: List all of the parameters and variables in the Lotka-Volterra model and what each stands for. Then, describe the difference between variables and parameters. Finally, list at least three assumptions of the Lotka-Volterra model and indicate which are idealizations.

5.7 Think: Suppose that you are modeling interactions between alligators (predator) and ducks (prey). Make a list of five features of the target system you think your model should take into account. Then, for each feature, say how it is similar or different in other predator/prey systems. For any features that are different, can you think of a related feature that would be similar between the systems?

5.8 Apply: Indicate at least three important differences between the Bay model and the Lotka-Volterra model, and describe a reason for each of the differences.

5.9 Recall: List the three steps of modeling outlined in this section and state the goal(s) of each.

5.10 Think: Recall how experiments involve the three steps of generating expectations, performing an intervention, and then analyzing the resulting data. State the three main steps in modeling, and describe the similarities between those and the three main steps in experimenting. Then, describe how modeling and experimenting are different.

5.11 Think: Suppose that we have a model of the Earth’s climate and we derive several predictions about average global temperature from the model. The assumptions of the model are somewhat unrealistic of the real-world climate, but replacing those assumptions with slightly different ones leads to similar predictions. Define robustness analysis and describe how this is an instance of it. What might you be able to learn from this invariance across the models?

### 5.3 VARIETIES OF MODELS

After reading this section, you should be able to:

- Define data model, say how data models are used, and list the three steps to construct one
- Give examples of models of these five types: scale, analog, mechanistic, mathematical, computer
- Describe the prisoner’s dilemma and iterated prisoner’s dilemma models and what scientists learned from each

 

#### Models of data  

The range of things that count as scientific models is extremely broad. Scientific models can be concrete physical objects, like the Bay model, mathematical equations, like the Lotka‐Volterra model of predator/prey interaction, or even implemented by computers. We’ll explore this variety in this section.

To start, let’s distinguish between models of phenomena and models of data. So far in this chapter we’ve only discussed models of phenomena, models used to represent target systems to investigate those systems. Models of data are also representations that are used in place of what they represent, but they represent data instead of target systems, and they play a different role in scientific reasoning by facilitating the use of data in testing hypotheses. A data model is a regimented representation of data, often with the aim of discerning whether the data count as evidence for a hypothesis.

Recall that data are any public records produced by observation, measurement, or experiment; because they are public, data enable observations to be recorded and compared. Thermometer readings, video recordings of capuchin monkey gestures, notes about the observed positions of planets, clicks on a website, and participants’ answers on a questionnaire are all examples of data. Such records are raw data, which must be processed before they are useful to scientists. For instance, the video recordings of monkey behavior would need to be edited, organized by time and day, and rendered into a software‐compatible visual format before they can be used to learn about monkeys’ gestures. This process of data correction, organization, and visualization results in a model of the data.

Data models are developed by first eliminating errors, then displaying measurements in a meaningful way, and finally extrapolating from those measurements to expected data for measurements that weren’t taken.

Consider measurements of the positions of a planet—say, Neptune—over a period of months. Those measurements will be influenced by more than Neptune’s position. They will also reflect some combination of human mistakes, flaws and limitations of instruments used, like a telescope, and inaccuracies from changing atmospheric conditions. Scientists can try to identify such errors by calibrating the telescope and recording the atmospheric conditions along with their measurement of Neptune’s position. This is called data cleansing: identifying and correcting errors in a data set by deciding which data are questionable and should be eliminated.

Then, the next step is to represent the data in a meaningful way. Data of Neptune’s position over a period of months may be visualized as charted points. Finally, these points can be used to draw a curve of Neptune’s progression. The points represent scientists’ measurements, and the curve represents the scientists’ best guess for Neptune’s path through the sky. This curve is the data model.

This is a complicated enough task that it has its own name: the problem of curve fitting. Curve fitting is extrapolating from a data set to expected data by fitting a continuous line through a data plot. The problem is that data, no matter how much you collect, are always consistent with multiple different curves. Suppose that you have data for two variables—say, air pollution and life expectancy in different cities—and you want to figure out the mathematical relationship between the two. That is, you want to learn how people’s life expectancy relates to the level of air pollution where they live. Your data are represented by the points in Figure 5.5. Figure 5.5 also shows different curves, representing different relationships between air pollution and life expectancy, each of which appears to fit the data pretty well. Put in terms of underdetermination, introduced in Chapter 3, the data underdetermine which curve best captures the relationship between these two variables.

There’s no easy answer to how scientists should decide which curve fitting the data is best. Finding the curve that best fits all data isn’t usually the best approach. Data models can fit the data too well; the problem with sticking too closely to the actual data is that those data are never perfect. There might be outliers, or values that deviate from the norm for one reason or another, and noise, influences on data that are incidental to the focus. Scientists want their data models to be better than the actual data they’ve collected. In the end, they must choose a data model based on their background knowledge about the phenomenon and what they want to use the data model for. New data can be used to check how well the selected data model works, just as models of phenomena can be calibrated with comparison to their targets. Big data approaches, discussed in Chapter 4, make data modeling even more challenging. A lot of data without a lot of background knowledge about the data makes it even more difficult to solve the curve-fitting problem.

#### Scale models

Data models are used in experiments and non-experimental studies, where the phenomena are investigated directly. In contrast, models of phenomena, our main focus in this chapter, are used to indirectly investigate phenomena: the model is studied in order to draw conclusions about its target. This can be especially useful when direct investigation, experiments and studies, aren’t feasible—like how the Reber plan would affect the San Francisco Bay—or scientists want to draw broad conclusions about a class of phenomena—like how predator and prey population numbers influence each other. As we’ve already seen with the Bay model and the Lotka‐Volterra model, there are many different types of models of phenomena.

**Scale models** are concrete physical objects that are downsized or enlarged representations of their target systems. Architectural models of buildings or urban landscapes are a familiar example; these are widely used in civil engineering. The Bay model is also a scale model. Its spatial scale is 1:1000 in length (one meter in the model represents 1,000 meters in the real world) and 1:100 in depth (one meter in the model represents 100 meters in the real world). The Bay model also has a shorter timescale; 14.9 minutes in the model represents a 24-hour day in the real world.

Other scale models are enlarged representations of their targets. In 1953, James Watson and Francis Crick announced the historic discovery that the structure of DNA is a double helix. Watson and Crick had spent a couple of years building scale models out of wire and tin plates in the shape of the building blocks of DNA. After several failures, the two scientists recognized, in part from the x-ray crystallography work of Rosalind Franklin and Maurice Wilkins, that a double helix structure best fit existing knowledge about DNA. Their model had a spatial scale of roughly 1 billion:1. That is, one centimeter in the double helix model represented one-billionth of a centimeter in a real DNA molecule. (See Chapter 13 for more discussion of this discovery, including Rosalind Franklin’s role.)

#### Analogical models

Analogical models are representations with features similar to focal features of a target system. The Bay model is also an analogical model, as it shares physical properties like tides and currents with its target. But analogical models don’t need to be concrete. An example of an abstract analog model is the computer model of the mind, which is based on formal similarities between computers and minds. Like computers, the human mind is often thought of as an information-processing system that can be described in functional terms—that is, without talking about its actual physical composition, or hardware, but referring to what it does. Like computers, minds can be understood in terms of the operations they carry out to solve certain tasks, or in terms of their software.

Another example of an analogical model—located somewhere between the Bay model and the computer model of the mind on the concrete/abstract spectrum—is the Monetary National Income Analogue Computer (MONIAC). Also known as a Phillips machine, MONIAC is a hydraulic model that uses water flow, like the Bay model, but to represent the British economy. William Phillips built MONIAC in 1949 using a collection of plastic tanks, each representing some aspect of the economy, connected by pipes and sluices and different valves. The machine used an old airplane motor to pump around dyed water, representing money, to simulate the flow of money in an economy. An overhead tank, representing a treasury, could be drained so that the water inside could flow to other economic sectors (education, healthcare, infrastructure and investment, savings, etc.). Water could be pumped back to the treasury tank to represent taxation and state revenue. Exports and imports could also be simulated by adding or draining water from the model. MONIAC is a physical model, but not a scale model. The British economy isn’t operated hydraulically, of course. MONIAC used water as an analog to money; changes in water level and flow were analogous to changes in amounts of money in and transfer among various sectors of the British economy. In its day, this was an amazingly accurate tool for studying how changes in different economic sectors affect others.

One type of analogical model is the mechanistic model, a model that represents the component parts and operations constituting some recurring process. Various processes in living organisms can be construed mechanistically, such as blood circulation, protein synthesis, and cellular respiration. A mechanistic model helps illuminate how the phenomenon depends on the orchestrated functioning of component parts. Mechanistic models can be physical structures, but most mechanistic models are schematic representations of structures, functions, and the relationships among them. For example, the mechanistic model of the cellular sodium-potassium pump depicted in Figure 5.8 is a generic representation of how sodium and potassium are exchanged through cell membranes.

Relying on analogies is a particularly useful strategy in early stages of modeling, when scientists may have little or no knowledge of the target system. This enables scientists to focus on the salient features of the target and to let the discovery of analogous features guide modeling approaches. For example, spiral staircases were an inspiration to Watson and Crick, guiding their modeling efforts of DNA toward a double helix structure. As knowledge about the target develops, analogical models may give way to models less obviously related to the target systems they represent.

#### Mathematical models

Mathematical models are mathematical equations that use variables, parameters, and constants to represent one or more target systems. The Lotka‐Volterra model is an example. It uses a pair of first‐order differential equations to represent changes in predator and prey populations over time. The first equation, d x /d t = α x − βxy, describes the fluctuations of a prey population, d x, over time, d t, where αx represents its exponential growth and β xy represents the rate of predator/prey interaction. The number of mice at a time, for example, is determined by their population growth, minus the rate at which they’re preyed upon by, say, hawks. In contrast, the number of hawks is fixed by their population growth given the supply of prey, minus their mortality rate. Hence, the second equation, d y /d t = δxy − γy, describes the fluctuations of a predator population, d y, over the same time interval, where δ xy represents predator population growth and γ y represents loss of predators due to events like death, disease, or emigration. (When we introduced the Lotka‐Volterra model earlier in this chapter, we said that x is a variable representing the prey population size and y is a variable representing the predator population size, while α, β, δ, and γ are parameters for, respectively, the prey population’s rate of growth without predation, the rate at which prey encounter predators, the predator population’s rate of growth, and the loss of predators by either death or emigration. Look at what we say about each part of each equation here with those definitions in mind to get a better sense for what each equation is used to represent.)

Another example of a mathematical model is a game theory model called the prisoner’s dilemma. Suppose that you and your friend Dominik have been arrested for robbing a bank, and you’ve been placed in different jail cells. A prosecutor makes this offer to each one of you separately:

You may choose to confess or to remain silent. If you confess and your accomplice keeps silent, all charges against you will be dropped, and your testimony will be used to convict your accomplice. Likewise, if your accomplice confesses and you remain silent, your accomplice will go free while you will be convicted. If you both confess, you will both be convicted as co-conspirators, for somewhat less time in prison than if only one of you is convicted. If you both remain silent, I shall settle for a minor charge instead.

Because you are in a different cell from your friend, you cannot communicate or make agreements before making your decision. What should you do? Assuming that neither of you want to be imprisoned, you face a dilemma. You will be better off confessing than remaining silent, regardless of what Dominik does. Look at the prior passage and think through what happens to your jail time if Dominik doesn’t confess and if Dominik does confess; in either case, it’s better for you if you confess. So, it looks like you should confess, right away! The problem is that Dominik is in the exact same situation. And the outcome where both of you confess is worse than the outcome where you both remain silent. If you and Dominik do the same thing, it’s better to both remain silent.

The prisoner’s dilemma seems to raise a puzzle for rationality. You are better off confessing, regardless of Dominik’s choice, but if you both are inspired by that fact to confess, you are both worse off than if you had both remained silent. Reasoning independently, it seems you should confess. But if you could plan what to do together, you’d both choose to remain silent. What to do?

The prisoner’s dilemma is usually represented using the mathematical formalism of game theory. The prisoner’s dilemma we described is depicted by the payoff matrix in Table 5.1.

Although this situation may seem contrived, numerous real-life scenarios can be modeled with a generic version of the payoff matrix, as shown in Table 5.1. The numbers represent generic payoffs or consequences of each decision. The higher the number, the more desirable the payoff. The first number in each set of parentheses represents Player 1’s payoff, the second number Player 2’s payoff. What’s important about these payoff numbers is just that the number for defecting is always higher than the number for cooperating, but the number when...

**Table 5.1** (a) Game theory payoff matrix for the prisoner’s dilemma (top); (b) Version of the prisoner’s dilemma generic to any choice of cooperating or defecting (bottom)

|      | Dominik | Remains Silent                | Betrays                        |                   |
| ---- | ------- | ----------------------------- | ------------------------------ | ----------------- |
|      | You     | get 3 years of prison         | Each pays a minor fine         | Dominik goes free |
| You  | go free | Each serves 2 years of prison | Dominik gets 3 years of prison |                   |

|          | Cooperate | Defect |        |        |
| -------- | --------- | ------ | ------ | ------ |
| Player 1 | Cooperate | (2, 2) | (0, 3) |        |
|          |           | Defect | (3, 0) | (1, 1) |

Both players cooperating is higher than the number when both players defect. In Table 5.1 b, 3 > 2 and 1 > 0, but 2 > 1. These numbers capture the dilemma: you do better if you defect regardless of your partner’s choice, but things will go better for you if your partner cooperates. The bank robbery scenario is just a vivid illustration; the prisoner’s dilemma can model lots of real-world situations involving people, businesses, nations, animals, or even bacteria. Any real-life scenario in which entities vary their strategy in ways that affect each other can be represented by the prisoner’s dilemma game if the desirability of the consequences can be represented by these payoff numbers.

The prisoner’s dilemma has been used to model many scenarios involving cooperation, in human societies and the biological world alike. For example, consider the symbiotic relationship of cleaner fish. Individuals of one species (the cleaner) remove parasites and dead skin from individuals of the other species (the client). Cleaner fish may choose to cooperate by cleaning the client fish or to defect by eating extra skin from the client fish. Client fish may choose to cooperate by allowing the cleaner fish to clean safely or to defect by threatening or eating the cleaner fish. Both fish types are better off if they cooperate: the client fish gets an important cleaning while the cleaner fish gets dinner. But there’s a benefit to defecting for each: the cleaner fish would get a bigger dinner by eating more from the client fish, and the client fish would get to eat the cleaner fish. The prisoner’s dilemma has been used to reveal the circumstances that can enable cooperative symbiosis like this to evolve.

#### Computer models

While many real‐world situations can be modeled as cases of the prisoner’s dilemma, what we’ve seen so far isn’t enough to show why businesses, gangsters, fish, and nations so often cooperate in real life. One important reason is that, in many real‐life scenarios, decisions about whether to cooperate aren’t made in an isolated room, separated from your partner. Instead, businesses, gangsters, fish, and nations all tend to signal their intentions, negotiate while making decisions, or interact repeatedly over time, allowing reputations to form.

The prisoner’s dilemma model can be extended to represent these kinds of interactions. One common extension is the iterated prisoner’s dilemma, where we suppose that two agents play the prisoner’s dilemma with each other repeatedly. This is one way in which cooperative behavior can win out over the selfish choice to defect. In the 1980s, a computer game provided insight into how this can happen. The political scientist Robert Axelrod invited social scientists to submit computer programs for a tournament of the iterated prisoner’s dilemma. Fun! Each program had its own strategy governing the circumstances in which it would cooperate or defect, and these programs were pitted against one another to see which would perform best in the long run. This tournament was a computer model.

In Chapter 4, we introduced computer simulations: computer programs developed from data about a phenomenon to simulate its behavior. Computer simulations are a variety of model; we can equivalently call them computer models. Axelrod’s tournament wasn’t a computer simulation of a specific phenomenon of interest but a simulation of iterated exchanges in which cooperation is valuable but there’s a temptation to defect.

By inviting open participation from other modelers, Axelrod made it so that the strategies deployed in his model weren’t limited by his own preconceptions, and the results did turn out to be surprising. The winning strategy, accumulating the most points in the iterated prisoner’s dilemma tournament, belonged to a program named Tit‐for‐Tat, submitted by psychologist Anatole Rapoport. The program was so simple that it had only a few lines of programming code. Tit‐for‐Tat cooperated in the first round of any game it played, and then it mirrored the other player’s previous action in every subsequent round. So, when Tit‐for‐Tat played against generally cooperative programs, it behaved cooperatively and reaped the rewards of that mutual benefit. But when Tit‐for‐Tat played against players that frequently defected, it too played selfishly after that initial cooperative move. This protected it from exploitation by selfish programs. Axelrod’s computer simulation thus demonstrated the strategic success of emulating the cooperation of others, which has been dubbed reciprocal altruism.

##### EXERCISES

5.12 Recall: Characterize models of data and models of phenomena, and give an example of each. How are these types of models similar? How are they different?

5.13 Apply: List the five types of models of phenomena described in this section, and give an example of each. For each example, indicate why it counts as a model of that type, and what target system(s) it is supposed to represent. Then, rank your examples from 1 to 5, where 1 is the most concrete relationship to the target system(s) and 5 is the most abstract.

5.14 Recall: List the three steps of data modeling. Then, describe the curve-fitting problem and indicate how it complicates those steps.

5.15 Think: Describe the prisoner’s dilemma and iterated prisoner’s dilemma models, and indicate what type of model each is. Considering both models, what do you take the target system(s) to be, and what do you think scientists can learn from these models?

5.16 Recall: Mathematical models are among the most abstract representations of target systems. Describe how mathematical models are nonetheless similar to target systems, using the example of the Lotka-Volterra model. (Returning to section 5.3’s discussion of the Lotka-Volterra model might be helpful.)

### 5.4 LEARNING FROM MODELS

After reading this section, you should be able to:

- Describe how models can play an experimental role and how they can play a theoretical role
- Identify three features that all models share
- List five desirable features of models and describe tradeoffs among these features

#### Modeling as experimentation and theorizing

Constructing and analyzing a model shares some similarities with experimentation. Both experimenters and modelers perform interventions to test expectations based on a hypothesis; like experiments, modeling can provide evidence for or against hypotheses about target systems. Expectations about the consequences of the Reber plan for the San Francisco Bay were tested with interventions on the Bay model. Animal models like the fruit fly are used to test expectations about the genetic causes of human diseases, like diabetes and lymphoma. And studying the iterated prisoner’s dilemma helps test expectations about the conditions that enable cooperative behavior to emerge among self‐interested individuals.

In each example, the models were used to test scientists’ hypotheses about real‐world systems, sometimes with surprising results. So, models can play a role similar to experiments. One important difference is that, with experiments, scientists intervene directly on the target system, whereas with models, interventions to models are used to draw conclusions about the target system. This is why models must accurately represent their targets.

Getting models to reflect their targets more accurately is a primary task of modeling. When a model is known to accurately represent its target, it can play a role similar to a scientific theory by representing core features of that phenomenon. When a model behaves similarly to the expected target system(s) in many instances and across different circumstances, it may become accepted as part of a theory of how the target behaves. An example of such a theoretical use of modeling is the Lotka-Volterra model of predator-prey interactions. If one sets the model’s parameters based on observations or estimates of specific predator and prey populations, one can then use the model to predict changes over time in the sizes of these populations. These predictions are a good account of how real predator-prey populations behave, and when they go wrong, one can usually figure out why by comparing the model’s features to the target system’s features.

So, models can play an experimental role by helping to investigate phenomena empirically, and they can play a theoretical role by providing an account of phenomena. Sometimes the same model can even play both experimental and theoretical roles. In Axelrod’s tournament, computer simulations served as virtual environments to test which strategies would perform best in an iterated prisoner’s dilemma game. While there were no expectations that Tit-for-Tat would win, this outcome accorded with an existing theory in evolutionary biology, namely reciprocal altruism. The basic idea is that it can be evolutionarily advantageous for an organism to help another at some cost to itself if there is a chance the favor will be returned in the future. The success of Tit-for-Tat was based on reciprocity, and so was consistent with this theory. Thus, the success of Tit-for-Tat in Axelrod’s computer tournament confirmed the idea in evolutionary theory that natural selection can favor cooperative behavior, even when there are costs, if the behavior is reciprocal.

#### Common features of models

This chapter has emphasized how different models can be from one another. Still, all models share three important features. First, all scientific models are used to learn about the world. Data models represent data to enable hypothesis-testing. Constructing and investigating models of phenomena enable scientists to reason about target system(s) in hopes of gaining new scientific knowledge. In both cases, the models are used as vehicles for learning about natural phenomena investigated in science.

Second, all models are used to represent: they are about, or stand for, the phenomena they target—or, for data models, the data they characterize. For a model to represent a target, it must be like the target in the right ways. Often, this likeness is understood in terms of similarity. But we have said that models aren’t exactly like the target systems they represent; they are also dissimilar from their targets in important ways—smaller or larger, mathematical or computerized. What similarities and differences are intended between a model and a target governs how the model should be interpreted and used.

Third, all scientific models involve idealization and abstraction. When constructing a model, scientists leave out some features of the target system and incorporate features the target does not have. Omitting or ignoring known features of the system is called abstraction. Misrepresenting features of the system with false assumptions is idealization (defined earlier). Abstraction and idealization can be used to set aside or simplify some features of target systems to focus on only those features deemed important for the purposes at hand.

The Lotka-Volterra model, for example, abstracts away from properties of prey and predators, like their speed, size, and capacity for camouflage. Those features aren’t essential to how predator-prey interactions influence population size and so have been abstracted, or removed, from the model. The Lotka-Volterra model also incorporates idealizations of predator-prey interactions. That model’s idealizations include the assumptions that prey can always find food, that predators are always hungry, and that both predators and prey are moving randomly through a homogenous environment. Scientists know these assumptions aren’t true, but they are helpful simplifications that usually don’t interfere with the model’s usefulness.

These three features of scientific models—their use to learn about natural phenomena, their representation of target system(s), and their incorporation of abstractions and idealizations—all relate to one another. Abstraction and idealization are features of models that affect how they represent their targets, and the ways models represent their targets partly determines what can be learned from them. Representation, then, is at the heart of scientific modeling.

#### What makes a model good?

We’ve seen that a target system can be represented in many ways. A physical model of a hydrological system, like the Bay model, represents water flow in ways that differ from mathematical models of fluid dynamics. And both of those are different from the computer model that eventually took over the work of the Bay model. There’s no one perfect model of a given phenomenon. Instead, the goodness of a model is determined by what the modelers want to learn from the model. Other factors, such as cost or ease of developing or using the model, can also be important. Sometimes one model will be enough for learning about a target system; other times, multiple models of the same target will be necessary to gain knowledge.

It is desirable for models to be accurate, general, precise, tractable, and robust. Each of these features helps make a model valuable, but attempting to maximize all of these features is futile. This is because these features trade off against one another: gaining more of one desirable feature of a model often requires losing ground on some other desirable features. For example, a model that is more general by applying to more target systems is also often less accurate of any one target system. This is because targets differ from one another in some regards. So, when constructing models, scientists must decide which desirable features to emphasize and which to compromise on.

##### Box 5.1 Values in Modeling

Models always are similar to their target systems—but are also different from them in various ways. This creates an opportunity, or an inevitability, really, for the influence of social values on scientific modeling. Recall from Chapter 2 that social values are the priorities and moral principles accepted in some community. One way social values influence modeling is in shaping what features of a target system scientists choose to represent accurately in a model and what features they ignore or distort. These choices depend on modelers’ aims. The influence of values on these choices is salient especially (but not only) for models in social science, as how a phenomenon should be defined and measured given a certain purpose and what factors are important to understand often relate closely to social values.

For example, philosopher of science Eric Winsberg has criticized predictive models of the Covid-19 pandemic that were used to justify governmental stay-at-home orders for including consideration of illness and death from Covid-19 but excluding consideration of the health impacts of school closures, social isolation, and deferred healthcare. There’s not one single right way to develop a model. What’s important is that similarities and differences from a target system be thoughtfully designed and open to scrutiny from others. Multiple models of one target phenomenon also can be developed; this was done for models of the Covid-19 pandemic, and it also occurs in climate change modeling and modeling of many other phenomena of widespread social importance. Multiple models of the same target can lay bare how modeling choices based on different aims and values influence what we understand about a phenomenon.

#### Accuracy

Accuracy is the extent to which a model represents the actual features of its target system; models that are more accurate better represent more features of a target system. A model that represented all and only the actual features of its target would be maximally accurate, but maximal accuracy is unhelpful; recall that models are improved by some differences from their targets. For example, the Bay model improved its representation of water flow by introducing inaccuracies of water depth. Overemphasizing a model’s accuracy can come at significant costs to tractability and generality—making the model difficult to develop and inapplicable to target systems that differ in minor ways.

#### Generality

Generality is a model’s applicability to a greater number of target systems; a model is more general when it applies to a greater number of target systems. Generality is a desirable feature of models insofar as it enables models to be reused in various circumstances. General models also highlight what various phenomena have in common with one another. Because the prisoner’s dilemma can apply to nations, squirrels, and pirates, this is a general model with numerous applications. This generality reveals something that these scenarios have in common: repeated interactions can enable cooperation to spontaneously emerge. However, sacrificing some generality in a model can be worthwhile, depending on the aim of the model, if this enables the model to represent its target more accurately. A general prisoner’s dilemma model might be supplemented with information about, say, how natural selection favors bacteria that can persist near one another (a form of cooperation). The resulting model will give more insight into bacteria cooperation in virtue of this additional detail. But it also will be less general—it will no longer apply to humans or corporations. Which is better depends on the modelers’ aims.

Precision is the extent to which a model finely specifies features of a target system; a more precise model more finely specifies features of the target. For example, climate models that allow scientists to predict how much warmer the global average temperature will be in 30 years within a range of ±0.05° Celsius are more precise than models that allow them to predict a ±1° Celsius range of temperature increase in 30 years. Notice that precision is different from accuracy. Whereas accuracy is a matter of a value’s proximity to the true value, precision is a matter of how finely specified a value is. So, a model can be very precise but inaccurate. Think of an archer loosing arrows at a target. Arrows that are scattered all around the bull’s eye but near it are accurate but imprecise. Arrows that are tightly clustered together but off center, away from the bull’s eye, are precise but inaccurate (see Figure 5.9).

Greater precision benefits a model by enabling it to give a more specific characterization of its target and to make more specific predictions. But increasing precision usually comes at the cost of a model’s generality, its tractability, and sometimes its accuracy. Highly precise models are less generally applicable and more difficult to develop. And, the more specific a prediction is, the easier it is for that prediction to be incorrect.

Tractability is the degree of ease in developing or using a model. More tractable models are easier to construct, manipulate, or analyze. This may involve different considerations, like the time it takes to run a model on a computer, whether the equations of a mathematical model have exact solutions, or even whether a modeler happens to be already familiar with one approach but not another. For example, because the iterated prisoner’s dilemma involves agents having repeated encounters, this model is less tractable than the original prisoner’s dilemma. One consequence of this decreased tractability is that scientists know exactly what the possible outcomes are for the original prisoner’s dilemma, but they cannot easily predict the outcomes for the iterated version. This is why Axelrod ran a computer tournament to explore some of the possible outcomes. Tractability is never maximized, though: the easiest thing to accomplish is nothing at all, and more complicated models regularly result in more accurate, precise, and useful findings. For instance, the iterated prisoner’s dilemma reveals how repeat encounters can overcome the dilemma, making cooperation directly beneficial.

Robustness is a measure of insensitivity to features that differ between a model and the target system, including abstractions and idealizations. Normally, scientists don’t want their model’s predictions to be influenced by those features, since they aren’t shared by the target. So, a more robust model is one that changes less despite variation in its assumptions. But limited robustness is inevitable. Models incorporate assumptions, including idealizations, that are needed for them to do the tasks they are designed for. If they didn’t matter, they wouldn’t be needed. What scientists aim to avoid is overreliance on specific assumptions that are unlikely to be true or even known to be false. Multiple models are sometimes used to determine how robust a model’s predictions are. If different models, with different assumptions, predict roughly the same result, that prediction seems more trustworthy than if it had been generated by just one model with uncertain assumptions and parameters. This is robustness analysis, introduced in section 5.2.

There is no single answer to how a model should best incorporate these desirable features, nor is there a perfect tradeoff among the features. Instead, scientists strategically develop their models to be tractable enough for their current circumstances; robust enough to be certain to some reasonable degree; accurate and precise enough to make interesting, trustworthy predictions; and general enough to be enlightening across the range of phenomena they are interested in. The balance struck thus depends in subtle ways on the phenomena under investigation, the scientists’ circumstances, and the purposes to which the models are put.

##### EXERCISES

5.16 Recall: Describe how models can play an experimental role and how they can play a theoretical role, giving an example of each.

5.17 Recall: List the three main features that scientific models share, and articulate how these three features relate to one another. Use one example model to illustrate all three features.

5.18 Apply: Locate and investigate a scientific model not already discussed in this chapter. Classify its type of model, and describe what target system(s) it’s used to represent. Describe how the elements of the model represent features of the target system(s) and what scientists have learned about the target system(s) from the model. With reference to ideas from this chapter, discuss why this model is a helpful way for scientists to investigate this phenomenon.

5.19 Recall: Define abstraction and idealization. What is the difference between them?

5.20 Think: Choose one of the models discussed in this chapter. Formulate a list of the abstractions involved in using the model to represent its target system and a separate list of the idealizations involved in using the model to represent its target.

5.21 Apply: In your own words, describe the five desirable features of models characterized at the end of this section. Then, for each feature, compare two models: the classic mathematical model of the prisoner’s dilemma and the computer model of the iterated prisoner’s dilemma. For each feature, write down whether you think one model is better and which one. Explain your answer.

##### FURTHER READING

For more on the use of models in science, see Weisberg, M. (2013). *Simulation and similarity: Using models to understand the world*. Oxford University Press.

For more on idealization, abstraction, and tradeoffs, see Potochnik, A. (2017). *Idealization and the aims of science*. University of Chicago Press.

For a discussion of computer simulation, see Frigg, R., & Reiss, J. (2009). The philosophy of simulation: Hot new issues or same old stew? Synthese, 169, 593–613.

For discussion of computer modeling and attention to climate change models, see Winsberg, E. (2010). Science in the age of computer simulation. University of Chicago Press.

For discussion of computational methods in science, see Humphreys, P. (2004). Extending ourselves: Computational science, empiricism, and scientific method. Oxford University Press.