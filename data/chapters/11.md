CHAPTER 11 - Causal reasoning

### 11.1 POVERTY AND THE CHARACTERISTICS OF CAUSAL REASONING

After reading this section, you should be able to:

- Describe the phenomenon of poverty and what the Niger experiment revealed about it
- List three characteristics of causal reasoning
- Give examples of how causal knowledge is of theoretical and practical importance

How can science help reduce poverty?

According to the United Nations’ Sustainable Development Goals Report in 2022, more than 650 million people worldwide live in extreme poverty, surviving on less than $1.90 per day. This number may get higher in the coming years, especially if economic and social inequalities, war, and the environmental consequences of climate change worsen.

Poverty doesn’t occur only in some remote regions of our planet. Poor people live in all countries and likely live in your neighborhood too. This is because poverty is not just about money. It is a complex phenomenon. Some of the salient aspects of poverty concern one’s economic resources like assets and income, but other aspects are medical, psychological, and social, like malnutrition, psychiatric conditions, and lack of opportunity.

When people experience poverty, basic human needs go unsatisfied. The poor can suffer from hunger, malnutrition, ill health, deficient education, and insufficient access to technologies, infrastructure, social networks, and cultural opportunities. For individuals, poverty can cause or worsen social isolation, shame, anxiety, and depression. For communities and nations, poverty affects migration trends, public health, and political stability. These consequences partly depend on social norms preventing certain households, such as immigrant households, or individuals, such as women, from seizing new economic opportunities or benefitting from the support of a social network. Given all these complexities, how can scientists help to reduce poverty and prevent or mitigate its negative effects?

One promising idea comes from Niger. Niger is one of the world’s poorest countries. In 2022, approximately half of its 25 million residents live in extreme poverty; in 2005, that number was 80%. In 2012, the government of Niger began giving small amounts of money to its poorest citizens. These small cash transfers had an obvious effect: they significantly improved the livelihoods of many of the poorest people. You may worry that people would use free cash in foolish ways. But several meta‐analyses from developmental economics have shown otherwise. Rather than buying what are called “temptation goods,” poor people typically use money received through a cash‐transfer or microcredit program to improve their condition and the condition of their families.

Though cash transfers are helpful, they do not address the lack of opportunities associated with poverty. Researchers collaborated with Niger’s government to determine whether the effects of its cash‐transfer program would improve by adding other components. The researchers conducted a randomized controlled trial, randomly assigning 322 villages in Niger to a control group or one of three experimental groups. Households across all villages in the control and experimental groups received monthly cash transfers. Households in all three experimental groups also received training in business and administration. And then, the three different experimental groups provided (1) an extra cash grant, (2) life‐skill training by social workers and a community film promoting communication, boosting aspirations, and noting women’s lack of opportunities compared to men in the same village, or (3) the extra cash as well as life‐skill training and the community film.

The researchers measured several variables related to the well‐being of the participants in this experiment, especially variables related to women’s empowerment, before starting the experiment and at 6 and 18 months afterwards. They also calculated the cost‐effectiveness of each intervention by comparing the monetary costs with the impact in each of the experimental conditions.

It turned out that the households in all conditions increased their consumption of food and domestic products. Households that also received psychosocial support showed improved levels of mental health, more positive expectations about the future, and increased social support, which in turn had further positive impacts on their economic situations over time. The researchers also discovered that integrating economic and psychosocial support to alleviate poverty had more lasting effects and low additional cost compared to simple cash transfers. This finding highlights that there is much more to poverty than a mere lack of money and that programs that target both monetary and psychosocial factors might be especially cost‐effective.

Experiments like this can provide economists and policymakers with evidence about the causes and effects of poverty, as well as evidence about the relative efficacy of different interventions to reduce poverty. Economic concepts like poverty can seem abstract, but causal knowledge in economics plays key roles in the formulation of social policies and design of institutions. Such policies and institutions have many impacts on people’s daily lives.

#### Three characteristics of causal reasoning

Causal knowledge is useful for goals such as alleviating poverty, mitigating global warming, preventing epidemics, and much more. But it can be tricky to uncover causal knowledge. Scientists have devised a number of methods to make reliable causal inferences in different experimental circumstances and for different theoretical and practical purposes. This chapter examines some of these methods for acquiring causal knowledge.

Chapter 3’s discussion of experimentation, Chapter 5’s discussion of modeling, and Chapter 10’s discussion of statistical hypothesis‐testing are all relevant to using experiments and models to test causal hypotheses, and we will draw from those discussions in this chapter.

The Niger experiment investigating the causes and effects of poverty illustrate three general characteristics of causal reasoning that apply both in science and to everyday life. First, causal relationships can be uncovered from information about the timing, location, and frequency of events. If you get sleepy after lunch, perhaps eating lunch is a cause of your drowsiness. In the Niger example, researchers measured variables related to poverty before and after interventions to establish whether outcomes associated with reduced poverty were more frequent after some interventions than others.

Second, testing causal hypotheses often involves doing something in the world, such as performing an intervention. Intervening on a suspected cause while keeping other variables the same or ensuring they vary at random can provide more insight into causal relationships than just observing a correlation in two variables. In the Niger experiment, scientists tested the roles of different interventions using multiple experimental groups receiving different interventions. Because long‐term economic prospects increased more, on average, in households receiving both monetary and psychosocial. support, this suggests that poverty is not just about money and that policies should concentrate also on psychological and social factors.

Third, causal reasoning has great theoretical and practical significance. Causal information is an important variety of scientific knowledge and can be used to explain features of our world, as we shall see in Chapter 12. Besides theoretical knowledge, information about causes is also how we can make things happen—and prevent things from happening—in the world. Causal knowledge is not only crucial for effectively reducing poverty but also for treating medical conditions, improving environmental outcomes, and much more.

##### EXERCISES

11.1 Recall Poverty is not merely a monetary deficit but a complex phenomenon. What are some of the causes of poverty? What are some of the negative effects that people deal with as a result of being poor?

11.2 Recall: The Niger experiment examined the effects on poverty of four experimental conditions. What were the four experimental conditions? What was the null hypothesis? What did the researchers find?

11.3 Think: Refresh your memory of external experimental validity, introduced in Chapter 3. Then, assess the external experimental validity of the Niger experiment described in this section. How confident can researchers be that these findings are accurate for residents of Niger? Should they be just as confident that the experimental results hold for poor households in other nations? Why or why not?

11.4 Recall: List the three characteristics of causal reasoning introduced in this section. Why is intervention useful for testing causal hypotheses? Why is causal knowledge more important than knowledge of correlations?

11.5 Apply: Provide your own example of (a) causal knowledge with theoretical importance and (b) causal knowledge with practical importance, and explain the importance of each.

11.6 Apply: Suppose you’re a lead scientist following up on the Niger experiment to see if you can similarly reduce poverty in your local community. However, you yourself have very limited financial resources with which to conduct your study; for instance, you have no money for cash payouts. What interventions could you make in your follow-up study?

### 11.2 THE NATURE OF CAUSATION

After reading this section, you should be able to:

- Indicate how spatiotemporal contiguity and correlation each can be clues to causation but fall short of ensuring causation
- Define the physical-process and difference-making accounts of causation
- Assess the strength of a causal relationship and indicate how causal background is relevant

Imagine that you are playing a game of billiards at your local pool hall. You hit the cue ball, which then rolls across the felt and strikes the 8‐ball, which is itself then set in motion. Did the cue ball cause the 8‐ball to move? The answer seems obvious. What else could have possibly made the 8‐ball move?

In the 18th century, philosopher David Hume asked what our experience allows us to say about the nature of causation. He suggested there’s no reason to say that causation is anything beyond regular associations between events. Hume’s argument goes as follows. If you were to observe the cue ball hit the 8‐ball, what you would have observed is just a series of events, one after another. You saw the cue ball moving towards the 8‐ball, the cue ball touching the 8‐ball, and then the 8‐ball itself moving. There is no additional experience of causes and effects in all of that. We should then conclude that there is no ingredient to causation beyond just the series of observable events that are regularly associated. (You may recall from Chapter 7 that Hume was also skeptical about inductive reasoning.)

Hume’s skepticism has motivated accounts of causation that go beyond mere regular association. Causal knowledge is important to science, but it’s tricky to say what, exactly, causation amounts to. This skepticism can also motivate us to clarify the specifics of reliable causal reasoning. What is it, exactly, that you would look for to decide, for example, whether poverty causes social isolation, or whether routinely eating processed meats causes cancer?

Let’s start by considering how spatiotemporal contiguity and correlation—together, Hume’s regular associations between events—relate to causation. The perception of causal relationships was systematically investigated by psychologist Albert Michotte in the 1940s. Michotte’s experiments showed that causal perception depends on spatial and temporal information. If two events—for example, pressing a piano key and hearing B‐sharp—are spatially and temporally contiguous, that is, if they happen at the same time and place, then we perceive them as causally related. This is so even if we only experience these occurring together once. When there is a spatial or a temporal gap between two events, we are much less likely to perceive the one event as causing the other. Spatiotemporal contiguity can be a powerful indicator of a causal relationship.

On the other hand, spatiotemporal contiguity can also be misleading. Not all events that occur together are causally related, of course: some events occur together once or even several times by pure coincidence. So, spatiotemporal contiguity doesn’t guarantee causation. Spatiotemporal contiguity is not necessary for causation, either. An effect can occur far away both in space and time from its cause. For instance, poverty in childhood can have effects much later in life, and poverty rates are influenced by geopolitical events that occur far away in time and space.

Indeed, many of the causal relationships investigated in science and important for everyday life are spatiotemporally separate to some degree. Sometimes the degree of separation is used to distinguish among an event’s causes. Proximate causes are those occurring more closely in time and place to the caused event, while distal causes occurred further away in time or place. For example, when asked about the cause of a friend’s poverty, you may describe her lack of social support and of a job at the moment, or you might note instead that many generations of her family have not accumulated much wealth and were denied access to social and economic opportunities. The former causes are proximate, while the latter distal. Proximate causes can be easier to identify, but distal causes are sometimes more important influences. This distinction between proximate and distal causes shows that events can have more than one cause. So, identifying a cause doesn’t mean that you have identified the only cause or the most important cause, or that you have ruled out other causes. We have seen that poverty has many causes, for example.

Besides spatiotemporal contiguity, we also tend to use information about correlation between events to discern causal relations. Indeed, correlation can be used in combination with spatiotemporal contiguity as a guide to causation. As we know from Chapter 9, two variables are correlated when the value of one variable raises or lowers the probability of the other variable taking on some value. Different events can occur together a few times by chance, but if the values of two variables are correlated across many instances, this is some reason to think one variable may causally influence the other. For example, there is a clear correlation between childhood poverty and many negative life outcomes. It’s thus worth investigating which (if any) of these life outcomes are caused by childhood poverty.

But correlation too is an imperfect guide to causation. For one thing, correlation is symmetric: if event A correlates with another event B, then B correlates with A as well. Causation isn’t symmetric. If studying for an exam and getting a good grade on the exam are correlated, this is because studying causes getting a good grade and not the other way around. Yet a correlation alone doesn’t provide information about which event is the cause and which event is the effect.

Further, correlations between events can exist even though neither event causes the other. Some events are correlated because they share a common cause—a third event that causes both. Famously, ice cream consumption is correlated with drowning, but eating ice cream is not a cause of drowning. Instead, there is evidence that the occurrence of hot days increases both ice cream consumption as well as swimming, and so also drowning rates. Finally, some correlations are spurious, that is, not causally related at all, such as the relationship between annual per capita cheese consumption and the number of people who died from becoming tangled in their bedsheets.

These are all ways correlations may fail to indicate causation. But the reverse can also happen: events can be causally related even when they are not correlated. Consider this example, due to philosopher Nancy Cartwright. Smoking cigarettes is well established as a cause of heart disease. Adequate exercise prevents heart disease. If smoking is strongly correlated with exercise, then this well-established cause of heart disease will also be strongly correlated with its prevention, and smoking and heart disease will not in general be correlated. This could be so even though smoking causes heart disease.

##### Box 11.1 Simpson’s paradox

In August 2021, less than a year after Covid-19 vaccines first became available, headlines announced the findings of a new study published in the prestigious journal Science: “Nearly 60% of hospitalized COVID-19 patients in Israel fully vaccinated.” The vaccine was supposed to help against Covid, but now more vaccinated people than unvaccinated were getting seriously ill! People worried that the vaccine’s efficacy waned very rapidly, no longer offering protection after a few months. But no: the vaccine was still very effective at preventing severe illness from Covid-19. Instead, this was an instance of Simpson’s paradox, where a correlation between two events disappears or is reversed when data are grouped in a different way. Israel had a very high vaccination rate: there were many more vaccinated than unvaccinated people. Further, age is correlated with the probability of vaccination: as in many other parts of the world, older Israelis were more likely to be vaccinated than younger Israelis. But older people are significantly more likely to suffer severe illness from Covid-19. The 40% of hospitalized Covid-19 patients who were unvaccinated was thus a much larger proportion at lower risk of severe illness than the 60% of hospitalized Covid-19 patients who were vaccinated. Accounting for the vaccination rate in the overall population and controlling for age revealed that Israelis over 50 were seven times more likely to be hospitalized if they were unvaccinated, while younger Israelis were 13 times more likely to be hospitalized if they were unvaccinated. There are many other real-life cases of Simpson’s paradox. Understanding how and why the paradox emerges is important for drawing correct causal conclusions from statistical evidence.

#### Accounts of causation: difference-making and physical processes

We have seen that spatiotemporal contiguity and correlation can be guides to causal relationships, but neither is a perfect guide. Despite Hume’s skepticism, there must be more to causation than mere association between events. Here are two ideas about what that might be.

One idea is that causal relationships are relationships of difference‐making. Put simply, the difference-making account of causation is the idea that if the occurrence of one event makes a difference to the occurrence of a second event, then the first event is a cause of the second event. If the billiard ball had not struck the 8‐ball, then the 8‐ball wouldn’t have moved. If the billiard ball had struck the 8‐ball in a different place or at a different speed, then the 8‐ball would have moved in a different direction or at a different speed. The billiard ball’s motion made a difference to the 8‐ball’s motion. Thus, the billiard ball’s motion caused the 8‐ball to move.

This difference‐making relationship goes beyond the mere correlation of events: a focus on intervention can help us see how. Recall that an intervention is a direct manipulation of the value of the independent variable. The idea of difference‐making is that, for one variable to count as a cause of another, an intervention on this variable, while controlling all other variables, will change the value of the other variable. A difference‐making account of causation uses the ideas of intervention and variable control to give an answer to what causation might be beyond mere correlation or association between events.

##### Box 11.2 Difference-making and counterfactual conditionals

According to the difference-making account of causation, causally relevant factors make a difference to whether an effect happens. The idea of difference-making relies on reasoning about counterfactual conditionals. Recall that a conditional is any if-then statement. Counterfactual conditionals have the following form: if C had occurred, then E would have occurred. For example, consider the conditional “if you had scored the penalty, then your team would have won the game.” If this counterfactual conditional were true, this would be reason to say that you not scoring a goal caused your team to lose. Such conditionals are called counterfactuals because the antecedent of the conditional is contrary to fact. Recall from Chapter 6 that material conditionals are false only when the antecedent is true and the consequent is false. This isn’t so for counterfactual conditionals. After all, the antecedent of a counterfactual conditional is always false. In our example, you didn’t score a goal, but that doesn’t guarantee your team would have won if you had—maybe that wouldn’t have been enough to win. It’s a matter of debate in philosophy how to assess whether a counterfactual conditional is true. It seems that to do so we need to consider how things would be in conditions unlike what has really occurred—in another “possible world,” philosophers sometimes say. The difference-making account of causation makes use of the concept of an ideal intervention to help with this. If we imagine intervening on what really happened to make it so that you scored a goal, keeping everything else exactly the same, then would your team have won? If so (and if you didn’t in fact win), then you not scoring a goal did cause your team to lose.

A second idea about the nature of causal relationships is that they are based on physical processes. On the physical process account of causation, causation occurs when there is a continuous physical process connecting a cause to its effect, such that a cause transmits its effect with the transfer of mass, energy, momentum, charge, or other physical properties. When the billiard ball knocked into the 8‐ball, some of its kinetic energy transferred to the 8‐ball, which is why the 8‐ball started moving. This is a physical process connecting the billiard ball’s motion to the 8‐ball’s motion. Thus, the billiard ball’s motion causes the 8‐ball’s motion. The physical process account gives a different answer than the difference‐making account to how causation goes beyond mere association, pointing instead to the transmission of physical properties.

The physical process and difference‐making accounts of causation may be compatible. But each account is more useful for thinking about causal reasoning in different circumstances. For some causal claims, physical processes are the benchmark; with others, they are difficult to track. Recall the research on alleviating poverty in Niger. How would you investigate energy transfer or other physical processes of the causes of poverty? You wouldn’t. In contrast, it’s clear how to think about increasing income with cash transfers, a suspected causal variable, and to test how those changes affect variables associated with poverty. This suggests a difference‐making account is apt.

For other causal claims, the idea of difference‐making doesn’t neatly apply. The moon orbits around the sun because of the curvature of space‐time. How could we intervene on this curvature to assess whether our intervention makes a difference to the orbit of the moon? It’s a bit confusing; this seems like a feature of reality that can’t be changed with an experimental intervention. And without even a way to conceive of such an intervention, we can’t use difference‐making to assess causal influence.

#### Strength of causation

Earlier in this section, we considered how correlation could provide clues to causal relationships. Correlation between variables occurs when the value of one variable raising or lowering the probability of the other variable takes on some value. These conditional probabilities are also useful in characterizing the strength of causal relationships.

Consider some cause, C, and its effect, E. To start at the extreme, if Pr(E | C) = 1, then the occurrence of a cause guarantees the effect. For example, having no income, family resources, or any other source of funds or necessities guarantees that someone will experience poverty. Having no source of funds or necessities is a sufficient cause of poverty, in the sense that this cause always brings about this effect. If Pr(E | not‐C) = 0, then the occurrence of a cause is necessary for the effect to occur. For example, a motionless 8‐ball will not roll across a flat pool table unless something strikes it. Another ball or other object striking an 8‐ball is thus a necessary cause of a motionless 8‐ball rolling across a flat pool table: a cause that must occur in order for the effect to come about. (See Chapter 6 for discussion of necessary and sufficient conditions, which is relevant to this use of necessity and sufficiency.) If both conditional probabilities hold, if both Pr(E | C) = 1 and Pr(E | not‐C) = 0, then the cause is both necessary and sufficient for the effect. The cause occurring guarantees the effect will occur, and without the cause occurring, there’s no way the effect will occur.

The strength of virtually all causal relationships falls short of these extremes. Most causes aren’t necessary or sufficient for their effects to come about. For example, though having no income can often result in poverty, family or governmental support could interfere with that effect. Having no income isn’t sufficient for poverty. Nor is it necessary: the working poor is a term for people who spend substantial time in paid employment and yet are experiencing poverty. A cause that raises the probability of an outcome occurring but does not guarantee the outcome is called a contributory cause, or partial cause, of that outcome. Most causes are contributory causes.

In general, you can judge the strength of a causal relationship with the following calculation:

S = Pr(E | C) − Pr(E | not‐C)

This measures the degree to which the occurrence of the cause increases the probability of the effect occurring by taking into account the likelihood of the effect when the cause does and does not occur. If more people without an income experience poverty and fewer people with an income experience poverty in the US compared to the Netherlands, for example, then we could say that lack of income is a stronger influence on poverty in the US than in the Netherlands. It’s important to notice that, although conditional probabilities can be used to characterize the strength of a causal relationship, they can’t identify when one variable is a cause of another. Recall from our previous discussion of correlation that simply identifying a correlation between the conditional probability of variables doesn’t mean one causes the other.

Most causes are merely contributory causes, rather than being necessary or sufficient for their effects, because causal relationships are often sensitive to other variables. The causal background consists in all the other factors that might causally influence an outcome, thereby also potentially affecting the causal relationship between the two events. The causal background for how lack of income causes poverty includes factors like family wealth, governmental support, savings, financial responsibilities, money management skills, and much more besides. This is one reason why poverty is a complex phenomenon. The conditional probabilities between a cause and its effect can change in different causal backgrounds or may only hold in some causal backgrounds.

Often, the causal background is ignored when causal claims are made. You do not generally consider that oxygen is a causally relevant factor when you explain how a defective stove caused an apartment fire, for example. But accounting for the relevant causal background is crucial in sound causal reasoning. For example, the value of cash transfers to mitigating poverty will depend on the causal background that shapes the extent to which lack of access to cash is responsible for experiencing poverty.

##### EXERCISES

11.7 Recall: How is spatiotemporal contiguity a clue to causation, and how does it fail to be a perfect guide to causation? How is correlation a clue to causation, and how does it fail to be a perfect guide to causation?

11.8 Apply: List three possible relationships that can result in a correlation between events A and B even though A does not cause B. Give a new example illustrating each.

11.9 Recall: Define the difference-making and physical process accounts of causation.

11.10 Think: What was Hume’s skepticism about causation? Evaluate the merits of his concern, taking into account the discussion throughout this section.

11.11 Apply: Define proximate causes and distal causes. Then, for each of the following questions, give one explanation that cites a more proximate cause and one explanation that cites a more distal cause. You might need to invent some details about these causal relationships to answer this question; that’s fine. Feel free to get creative.

1. Why did the Titanic sink?
2. Why did Ruth leave a tip after her meal at the restaurant?
3. Why did the hurricane happen?

---

11.12 Apply: Write down the formula that gives the strength of causal relationships. Then, considering that formula, order the following causal relationships from strongest to weakest, giving a brief rationale for the order of each item.

1. Brushing your teeth, flossing, and visiting the dentist prevents cavities.
2. Frequent smiling increases well-being.
3. Eating pizza prevents getting the flu.
4. Consuming anabolic steroids improves physical strength.
5. Increase in the minimum wage produces higher attendance at football games.
6. Warmer summers lead to longer periods of drought.

11.13 Think: For each of the causal relationships in 11.12, name one feature of the causal background that would make the causal relationship stronger and one feature of the causal background that would make the causal relationship weaker. It might help to consider the conditional probability relationship that gives the strength of causal relationships.

### 11.3 TESTING CAUSAL HYPOTHESES

After reading this section, you should be able to:

- Characterize how the difference‐making account of causation relates to intervention
- Describe how intervention, direct and indirect variable control, random sampling, and statistical hypothesis‐testing is each important to discerning causal relationships
- Articulate how statistical hypothesis‐testing is used to test causal hypotheses

#### Experimental interventions and difference-making

Discovering causal relationships requires more than just passively observing what happens. The idea of causation as difference‐making is useful for making sense of various methods scientists can use to acquire knowledge of causes. One such method is to run an experiment—ideally, a perfectly controlled experiment, as detailed in Chapter 3.

Suppose that you are a farmer interested in learning whether using a new fertilizer will increase your crop yield, that is, make a difference (positive difference) to the yield. This is a causal hypothesis. How would you test it? One way would simply be to try out the fertilizer on your crops and see what kind of a yield you get. But the causal background might vary from last year to this year in a way that affects crop yield: rainfall, temperatures, last frost date, and more. You wouldn’t be able to distinguish that influence from the specific effect of the fertilizer on the yield.

A better approach would be to divide your field into different plots of equal size. You can then use the new fertilizer on some plots but not others and compare the crop yield from the fertilizer plots to the crop yield from the other plots. If the plots treated with the fertilizer produce, on average, a larger crop yield than the other plots, then the fertilizer made a difference; we could then say the new fertilizer causes increased crop yield. If the two groups of plots yielded about the same amount of crop, then the new fertilizer is probably ineffective. If the fertilizer plots do worse, the fertilizer makes a difference—but the wrong kind!

Applying concepts introduced in Chapter 3, we can say that the farmer has created an experimental group (plots to which the fertilizer is applied) and a control group (plots handled according to the farmer’s past practices). The application of fertilizer to plots in the experimental group is an intervention. The farmer intervenes on a suspected cause to see whether this makes a difference to the suspected effect. The suspected cause is the independent variable, and the suspected effect is the dependent variable. The causal background consists of extraneous variables. Some extraneous variables are directly controlled by comparing yield in the same season, in the same field, with the same irrigation, and so on, while other extraneous variables are indirectly controlled by random assignment of plots of land to experimental group and control group.

Some experiments testing causal hypotheses aim to establish whether there is a causal relationship, and others aim to clarify the strength of a causal relationship. For example, some drug trials simply seek to establish safety: that a drug won’t have negative effects. Others seek to establish efficacy: that a drug will have the expected positive effect. And still others aim to establish the relative strength of a causal relationship: whether some drug is more effective than another already on the market.

Experimental interventions introduce an external influence on a system, which can help disentangle causal relations. This is why the suspected cause is called an independent variable—the intervention independently determines its value, which, if all goes well, eliminates the possibility that the suspected cause is affected by the causal background. Other features of experimental design, such as using random sampling to select experimental participants and having a control group, are designed to minimize the chance that changes to the suspected effect are due to the causal background instead of the intervention. Altogether, these features help scientists test causal hypotheses, identifying whether a particular factor is a genuine difference-maker.

#### Statistical variation and testing causal hypotheses

In experimental tests of causal hypotheses, hypotheses about the existence, direction, and strength of causal relationships are used to develop specific expectations regarding how dependent variables will change in response to changes to independent variables. But testing causal hypotheses is complicated by extraneous variables and chance variation. The experimental techniques of intervention, direct and indirect variable control, random sampling, and statistical hypothesis-testing are motivated in large part by their ability to discern causal relationships from mere correlation, cause from effect, and causal influence from chance variation.

We just saw how intervention and variable control are useful. Let’s now consider how statistical hypothesis-testing and random sampling help to test causal hypotheses in the face of statistical variation. In the fertilizer experiment we imagined earlier, we expected the new fertilizer to improve crop yield in the plots to which it is applied. Does this mean we should expect all fertilized plots to have better yield compared to all the standard plots? Surely not; varying causal background, and perhaps sheer random variation, will introduce variability among the plots. So, then, how many of the fertilized plots need to have a better yield to show that fertilizer makes a difference? To answer that question, we’d need to use inferential statistics.

As this illustrates, statistical hypothesis‐testing is crucial for testing causal hypotheses. Inferential statistics can be used to generate specific expectations of a group’s statistical properties, which is important for hypotheses that predict probabilistic causal influence. As we’ve seen, this is much more common than causes guaranteeing their effects. In null hypothesis significance tests, causal hypotheses play the part of the alternative hypothesis. The null hypothesis is usually that the posited cause does not actually influence the phenomenon of interest.

##### Box 11.3 Mill’s methods

In the late 19th century, the English philosopher John Stuart Mill emphasized the role of observation and experimentation in discerning causal relationships. Mill identified five methods to evaluate causal hypotheses. These can still be useful, though statistical analysis and intervention are both important tools of causal hypothesis-testing not captured by Mill’s methods.

1. Method of concomitant variations: Observing that the value of one variable changes in tandem with changes to the value of another variable. In modern terms, this just is using correlation between variables to infer that the variables might be causally related. Of course, mere correlation does not guarantee causation.
2. Method of agreement: Considering cases where an effect occurs to see what they have in common. If there is a prior event or condition common to all, then one may infer that this event or condition caused the effect.
3. Method of difference: Considering cases where an effect does not occur to see what distinguishes those from when the effect occurs. If an event or condition present when the effect occurred is absent when the effect does not occur, then one may infer it caused the effect.
4. Joint method of agreement and difference: Considering cases where the effect occurs to see what they have in common (method of agreement), as well as considering cases where the effect does not occur to see what was different (method of difference). If an event or condition common across cases where the effect occurs is absent when the effect does not occur, one may infer it causes the effect.
5. Method of residues: Comparing cases in which a set of causes brings about a set of effects to cases in which some of those causes bring about some of those effects and inferring, on that basis, that the absent cause(s) are responsible for the absent effect(s). Unlike the other methods, this is a way to reason about which of a set of causes are responsible for which effects.

So, for our farmer, the null hypothesis is that the fertilizer is causally inefficacious: the range of crop yield from the fertilized plots of land will only differ from the range of crop yield from the other plots by chance variation. The mean crop yield and crop-yield variation in the control group, together with the sample size, can be used to calculate the probability distribution for the experimental group’s crop yield assuming the new fertilizer does not make a difference. If the measured crop yield is sufficiently unlikely, we can reject the null hypothesis and conclude the new fertilizer causes increased crop yield.

Another complication of expectations from a causal hypothesis due to causal background relates to external experimental validity, that is, the extent to which experimental results generalize from the experimental conditions to other conditions. For example, should we expect the new fertilizer to have the same effect as in our experiment in a rainier season, when the causal background is different? Due to considerations of external experimental validity, it’s not sufficient just to ensure extraneous variables do not vary systematically between the experimental and control groups. Conclusions about a causal hypothesis may also go wrong when the causal background of an experiment is not sufficiently similar to the causal background in circumstances in which we seek to apply the causal knowledge.

Random sampling is helpful on this front. A sufficiently broad range of conditions that are randomly selected enables us to expect the range of experimental conditions to be similar to the real-world circumstances in which the causal knowledge would apply. This increases external experimental validity and, in turn, the relevance of our causal knowledge. Perhaps our farmer will have to test out the fertilizer over several seasons or on plots arranged to mimic expected variation such as in rainfall and temperature to know the new fertilizer causes increased crop yield across the range of growing conditions the farmer is likely to encounter.

##### EXERCISES

11.14 Recall: Describe the importance of intervention for testing causal hypotheses. Watch Video 15. Then, define the difference-making account of causation and characterize how it relates to intervention.

11.15 Apply: Headlines in popular media often misrepresent the scientific studies they discuss. In particular, many headlines suggest a causal relationship where the evidence provided by the scientific study only supports a correlation. Consider the following headlines. For each, (a) identify whether it makes either a causal or a correlational claim; (b) rewrite any headline using causal language so that it reads as a correlational study; and (c) suggest a possible explanation for each correlation that is not the posited or suspected causal relationship.

1. “Lack of Sleep May Shrink Your Brain” (CNN, September 2014)
2. “To Spoon or Not to Spoon? After-Sex Affection Boosts Sexual and Relationship Satisfaction” (Science of Relationships, May 2014)
3. “Daytime TV (Soap Operas) Tied to Poorer Mental Scores in Elderly” (Reuters, March 2006)
4. “Study Suggests Attending Religious Services Sharply Cuts Risk of Death” (Medical Xpress, November 2008)
5. “Facebook Users Get Worse Grades in College” (Live Science, April 2009)
6. “Texting Improves Language Skill” (BBC, February 2009)
7. “Study Suggests Southern Slavery Turns White People Into Republicans 150 Years Later” (Think Progress, September 2013)
8. “Dogs Walked by Men Are More Aggressive” (NBC News, November 2011)
9. “Want a Higher GPA? Go to a Private College” (New York Times, April 2010)
10. “Sexism Pays: Men Who Hold Traditional Views of Women Earn More Than Men Who Don’t” (Science Daily, September 2008)

11.16 Apply: Choose two of the headlines from Exercise 11.15, and look up the text of both. Write a paragraph evaluating the strength of the evidence cited in the media report supporting the claim (causal or correlational) in the headline.

11.17 Recall: Describe why each of the following is important for discerning causal relationships: direct and indirect variable control, random sampling, and statistical hypothesis-testing.

11.18 Apply: How would you design an experiment to determine whether smoking marijuana causes schizophrenia?

1. a. Identify the intervention and describe how you would control extraneous variables.
2. b. Identify the expectations given the hypothesis, that is, what finding would enable you to conclude that smoking marijuana is a cause of schizophrenia.
3. c. Describe how statistical hypothesis-testing and random sampling are employed and why each is important.

### 11.4 CAUSAL MODELING

After reading this section, you should be able to:

- Describe how causal modeling can yield causal knowledge
- Define causal Bayes nets and describe how they are developed and applied
- Identify three assumptions of causal Bayes nets and discuss their significance

#### From correlation to causation

Besides controlled experiments and statistical hypothesis‐testing, another important approach to gaining causal knowledge is causal modeling. Causal modeling involves the use of computational and statistical methods for representing, manipulating, and testing causal hypotheses. When experimental interventions cannot be conducted, causal modeling can be used to evaluate causal hypotheses. Even when interventions can be made, combining experiments with causal modeling can be used to evaluate whether correlational evidence supports the causal hypothesis and to help identify factors in the causal background that must be controlled during an experiment.

Like controlled experiments, causal modeling is also related to the difference-making account of causation, which defines causal relationships in terms of potential intervention and variable control. If an event C causes another event E, then an intervention on C influences the value of E. If C and E are merely correlated, share a common cause, or if E causes C, then interventions on C won’t causally influence the value of E. In causal modeling, difference-making is identified from patterns of conditional probabilities between variables representing different features of a target phenomenon. Scientists can leverage these patterns of conditional probabilities to represent causal structure and reason about how variables influence one another.

The method of regression analysis, introduced in Chapter 9, is one of the oldest causal modeling procedures. It is used to estimate the correlation of two variables, conditional on all other measured variables. It’s like drawing a best-fitting line for the relationship in the values of two variables based on data on a scatterplot. For a suspected causal relationship, regression analysis is used to estimate how a causal variable affects another variable. Merely running a regression analysis, however, cannot deliver causal knowledge when causal influence isn’t yet known. We need additional evidence from meta-analyses, experiments, or non-experimental studies to supply that information, then regression analysis can be used to estimate the nature of the causal effect, including its direction and strength.

As in regression analysis, the building blocks of many causal modeling approaches are statistical correlations and probabilistic dependencies between variables that represent different features of a target phenomenon. In causal models, variables represent potential causes and effects. Here we will focus on Bayes networks, or “nets.” A Bayes net is a kind of causal model that uses joint probability distributions to provide a compact, visual representation of causal relationships and the strength of those relationships. Nodes in the graph stand for variables of interest, and arrows connecting different nodes stand for hypothesized causal relationships between variables.

Conditional probabilities specify how each variable depends on its direct causes in a joint probability distribution: the probability distribution for each of a set of variables, taking into account the probability of the other variables in the set. These causal models are called Bayes nets because they use Bayes’s theorem to update the probabilities based on new information about the value of variables or their probabilistic relationships. (Bayes’s theorem was introduced in Chapter 8 and Bayesian statistics discussed in Chapter 10.)

#### Developing Bayes nets

To better understand how scientists use Bayes nets to learn about causal relationships, consider this scenario from an introduction to the main concepts and applications of Bayesian networks:

Suppose that a patient has been suffering from shortness of breath (called dyspnea) and visits the doctor, worried that he has lung cancer. The doctor knows that other diseases, such as tuberculosis and bronchitis, are possible causes of this symptom, as well as lung cancer. She also knows that other relevant information includes whether the patient is a smoker (increasing the chances of cancer and bronchitis) and what sort of air pollution he has been exposed to. A positive x‐ray would indicate either TB or lung cancer.

There’s plenty of causal information here, but how that information relates to the case at hand is tricky to determine. Doctors can use causal Bayes nets to generate medical diagnoses. To construct such a model, the relevant variables first need to be identified. Each variable is represented with a node. While there’s no uniquely right way of setting up the Bayes causal net, it helps to make choices about what nodes to include that enable us to represent the relevant aspects of the situation with enough detail to perform the desired reasoning. One possible modeling choice is shown in Table 11.1. In this case, the variables include dyspnea, smoker, pollution exposure, age, x-ray result, tuberculosis, and lung cancer.

The second step of constructing a causal Bayes net is to specify the system’s causal structure by drawing arrows between the nodes. If one variable affects another, then the corresponding nodes should be connected with an arrow indicating the direction of the effect. Smoking and living in a polluted area are two factors affecting the patient’s chance of having lung cancer. In turn, having lung cancer is a factor affecting the result of an x‐ray and the patient’s difficulty in breathing. If this is the structure of the situation, then we may draw the graph pictured in Figure 11.4.

Causal relationships represented in a causal Bayes net can take several forms. Causes can increase or decrease the probability of some variable taking on a given value, and there can be a feedback loop where two or more variables influence one another in a cyclical way. Most of the time, however, Bayes nets are assumed to be directed acyclic graphs (sometimes abbreviated DAG), which means that all the causal relationships are taken to go in one direction without feedback loops. This assumption means that earlier causes are not also later effects. You can see from Figure 11.4 that our graph makes this assumption; no arrows form circles like X → Y → Z → X, and no arrow is bidirectional like X ⟷ Y.

##### Possible values for variables in the dyspnea case

| Variable     | Values               |
| ------------ | -------------------- |
| dyspnea      | {T, F}               |
| smoker       | {T, F}               |
| pollution    | {low, high}          |
| x-ray        | {positive, negative} |
| lung cancer  | {T, F}               |
| tuberculosis | {T, F}               |

---

##### A causal graph for the dyspnea case

##### FIGURE 11.4

##### Conditional probabilities of developing lung cancer given level of pollution exposure and whether one smokes

| Pollution | Smoker | Pr(cancer = T | pollution, smoker) |
| --------- | ------ | ------------- | ------------------ |
| high      | T      | 0.050         |                    |
| high      | F      | 0.020         |                    |
| low       | T      | 0.030         |                    |
| low       | F      | 0.001         |                    |

After specifying the nodes and their structure, the strength of the relationships between connected nodes must be specified. To do so, one needs to define a probability distribution for each node, conditional on any node(s) that causally influences it. In the example of patient diagnosis, statistical information from previous medical studies or from observed frequencies are used to specify these probability distributions. For variables with no such information available, initial probabilities can be based on an intuition, guess, or estimation about the case. Recall from Chapter 10 that Bayesian statistics is supposed to make these initial guesses unimportant in the long run. Bayes nets allow us to infer conclusions even if we start off with imprecise or inaccurate initial probabilities.

Let’s look at the node cancer in Figure 11.4. Its parent nodes are pollution and smoker, each of which can take two possible values for a total of four combinations of joint values: {# ; # ; <low, T>; <low, F>}. We can then specify the conditional probability of having cancer for each of these four cases. One way to represent these conditional probabilities is in a table, as in Table 11.2.

#### Reasoning with Bayes nets

Once all conditional probability distributions are determined, our causal Bayes net captures the relevant knowledge available. Now we can start to reason with it. Reasoning with a Bayes net amounts to computing posterior probability distributions for one or more nodes of interest given the values of nodes that you have information about. These computations are governed by Bayes’s theorem or other algorithms for computing approximations to posterior probabilities. Think of this as updating your beliefs about the value of a node based on changes to your beliefs about the values of other nodes. The arrows connecting nodes show the paths that probability distribution changes follow.

Belief updating can happen either from cause to effect or vice versa. For example, if we’re certain that the patient has dyspnea, and her x‐ray results are negative, then we can update our diagnosis about whether the patient has cancer, a causal influence on both dyspnea and x‐ray results. In turn, updating our diagnosis about whether the patient has cancer will affect our beliefs about whether the patient smokes and lives in a high‐pollution area, proceeding up the chain of causal influence. Or, if we are certain that the patient smokes, we can update our beliefs about her chance of having lung cancer accordingly, which is causally influenced by smoking status. This also influences our expectations of the x‐ray result.

A different type of reasoning with causal Bayes nets regards the relationship between two causes that compete to explain an observed effect. In our case, smoker and pollution are two such causes. They compete to explain the value of the variable cancer, which they both influence. Suppose we learn that the patient has cancer. This new information raises the probability of both possible causes. Suppose that we learn further that the patient lives in a badly polluted city. Something interesting would now happen in our causal Bayes net. This new piece of information both explains the patient having cancer and lowers the probability that the patient is a smoker. Although the variables smoker and pollution are initially probabilistically independent, given that we know that the patient has cancer and lives in a highly polluted area, the probability that the patient is a smoker goes down. Knowing that the patient has been exposed to significant pollution accounts for the lung cancer and thus disrupts the probabilistic association between lung cancer and smoking. Now we needn’t speculate that the patient was a smoker in order to explain the lung cancer.

In the simple case we’ve considered, a Bayes net is specified and then used to make causal inferences and predictions. In many other scientific applications, causal Bayes nets are incomplete in two ways. First, there are many variables that could be added that would precede, mediate, or follow the variables explicitly represented in the model. Second, information might be lacking about the causal relationships between the variables represented in the model. In that case, the structure of the network and the relevant probabilistic dependencies must be learned from data, since defining a complete Bayesian network would be too complex. For this, scientists rely on computational methods like machine learning algorithms that search correlational data for causal dependencies.

Cognitive neuroscientists, for example, are interested in the causal relationships between brain areas that support a cognitive capacity. To find out about these causal relationships, they often rely on brain imaging data, where subjects perform tasks that tap the cognitive capacity of interest while having their brain activity recorded. Neuroscientists use background knowledge about which brain regions might be involved in a task to focus their attention on activity in a few regions of interest, each one of which can be treated as a variable and represented as a node in a DAG. The challenge is then to discover the causal structure of these regions. Machine learning algorithms help neuroscientists to tackle this challenge. An algorithm searches the brain imaging data set to find the causal structure that best explains observed statistical dependencies between the variables of interest. Using DAGs, it is then possible to determine whether a causal structure is in principle identifiable from a probability distribution and to derive the probabilistic expression for this quantity. 

#### Assumptions of Bayes nets

Reasoning with causal Bayes nets requires making several theoretical assumptions; these assumptions are needed to infer causal relationships from data about conditional probabilities. Three key assumptions are the common cause principle, modularity, and the causal Markov condition. These assumptions are not always satisfied by a data set, but when they are, causal Bayes nets are especially promising for learning about causal relationships between variables from their observed statistical features.

We encountered the idea of a *common cause* earlier in this chapter. The common cause principle says that every correlation between two variables is either due to a direct causal effect linking the correlated variables or is brought about by a third factor that causes both, that is, a common cause. This idea is of central importance in causal explanation and causal modeling. For example, suppose that two lamps in your room go out suddenly and simultaneously. You may look for whether the common power supply was interrupted. This interruption would be the common cause of the two simultaneous events and would explain this improbable coincidence.

**Modularity** is an assumption about how systems can be manipulated; it implies that interventions into causal relationships in a system should not change other causal relationships in the system. When a system is not modular, interventions on some causal relationship(s) change the nature of other causal relationships. If a system is modular, then dependencies between variables in a causal Bayes net model of the system that are not directly manipulated should not change. Modularity is a useful feature, as it allows for precise and focused predictions about what would happen in the target system if certain causal relationships were manipulated. When the modularity assumption is violated, Bayes nets won’t correctly specify the state of a system after an intervention.

Closely associated with modularity is the *causal Markov condition*. One of the most important assumptions of causal Bayes nets, the causal Markov condition is the requirement that the probability of causal variables conditional on their parent causes are probabilistically independent of all their other ancestors. The basic idea is that remote causes are irrelevant to conditional probabilities, and thus to causal inference, when we know the immediate causes of an effect. In our medical diagnosis example, the value of tuberculosis is influenced by the value of cancer, but probabilistically independent of pollution and smoker conditional on cancer. This is because cancer and tuberculosis are causally related, whether the cancer was caused by smoking or by pollution.

The causal Markov condition is motivated by the idea that, when probabilistic dependencies are found between variables, these dependencies are due to one variable causing the other or to their sharing a common cause. The causal Markov condition specifies which variables will be probabilistically independent conditional on other variables in a set of variables under study. If the causal Markov condition holds of a set of variables associated with a system, then conditional independence between variables indicates the absence of a causal relationship. For example, a causal chain like X → Y → Z implies that X, Y, and Z are all probabilistically dependent on one another but that X and Z should be probabilistically independent if Y takes a fixed value. In some cases, the Markov condition might fail if the set of variables included in the Bayes net omits common causes or includes variables that aren’t relevant causes.

These are three key assumptions underlying reasoning with causal Bayes nets, though there are others as well. Understanding how different strategies for causal modeling work when some of their assumptions fail, and what kinds of errors they would yield, are two of the most important challenges of current causal modeling approaches.

##### EXERCISES

11.19 Recall:  Explain what causal modeling is good for, pointing out its advantages and limitations.

11.20 Apply: For each of the following cases, (a) indicate the causal hypothesis involved, distinguishing causal variables from effects; (b) offer another plausible cause for the effect; and (c) explain whether the reasoning described in the case is good or bad with the help of a simple causal model (consider using a directed acyclic graph, or DAG).

​	a. You have eaten your birthday dinner at your favorite pizzeria in town for the past 10 years. This year you got sick. This was also the first time your Uncle Sam was there. You conclude you got sick because Uncle Sam was there.

​	b. Eryka normally goes to bed at midnight and gets up by 7:00 am each morning. She usually runs two kilometers after having some breakfast. This morning, however, she ran only half a kilometer and had to stop, as she was so tired. She recalled that she went to sleep unusually early the night before and concludes that too much sleep made her too tired to run.

​	c. Phineas Gage’s moral character changed dramatically after an explosion blew a tamping iron through his head. Gage was leading a railroad construction crew near Cavendish, Vermont, when the accident occurred. “Before the accident he had been a most capable and efficient foreman, one with a well-balanced mind, and who was looked on as a shrewd smart business man.” After the accident, he became “fitful, irreverent, and grossly profane, showing little deference for his fellows. He was also impatient and obstinate, yet capricious and vacillating, unable to settle on any of the plans he devised for future action.”

11.21 Recall: Describe (a) what a causal Bayes network is, (b) how a causal Bayes net is developed, and (c) how it is applied.

11.22 Apply: Consider the following story: A group of psychologists is interested in how intrinsic motivation of university students affects their exam results. They believe that intrinsic motivation affects both class attendance and home preparation (reading the textbooks, doing the assignments, etc.). They also believe that both class attendance and home preparation affect exam results. They do not believe that there are any further causal interactions. All relevant variables (intrinsic motivation, class attendance, home preparation, exam results) are considered to have two values: “High” and “Low” for intrinsic motivation, class attendance, home preparation; and “Pass” and “Fail” for exam results. The psychologists observe the following frequencies:

- 40% of all students have a high intrinsic motivation.
- 90% of all highly motivated students attend classes regularly, as opposed to 60% of all students with low motivation.
- 70% of all highly motivated students prepare well, as opposed to 20% of all students with low motivation.
- 80% of all students who prepare well and attend class regularly pass the exam.
- 60% of all students who prepare well and do not attend class regularly pass the exam.
- 45% of all students who do not prepare well and do attend class regularly pass the exam.
- 40% of all students who do not prepare well and do not attend class regularly pass the exam.

On the basis of this information, find the conditional probabilities and draw the causal Bayes net that corresponds to the story.

11.23 Recall: Describe the assumptions of common cause principle, modularity, and the causal Markov condition, and indicate why each is important for causal modeling.

11.24 Apply: Suppose that we measure the variables storm (S), barometer reading (B), and atmospheric pressure (A). You find that S and B are dependent, as are both B and A and S and A. Furthermore, you find that S and B given A are independent. From these constraints alone, what causal structure can you infer? Draw a simple DAG showing the causal relationship. Then, explain the role of the common cause principle and Markov condition in making reliable inferences about the causal relationships in the example.

# FURTHER READINGS

For more on poverty, see Lister, R. (2021). *Poverty. John Wiley & Sons. A more concise treatment is Wolff, J., Lamb, E., &  Zur‐Szpiro, E. (2015). A philosophical review of poverty*. Joseph Rowntree Foundation. www.jrf.org.uk/report/philosophical‐review‐poverty

For more on normative and descriptive accounts of causation and their importance in scientific explanation and psychology, see Woodward, J. (2021). *Causation with a human face: Normative theory and descriptive psychology*. Oxford University Press.

For a pluralist view of the nature of causation and discussion of causal analysis, including causal Bayes nets, see Cartwright, N. (2007). *Hunting causes and using them: Approaches in philosophy and economics.* Cambridge University Press.

For a short introduction to causal modeling, with a focus on the epistemology of causation, see Eberhardt, F. (2009). Introduction to the epistemology of causation. *Philosophy Compass*, 4(6), 913–925.

For an advanced treatment of causal modeling, see Pearl, J. (2009). *Causality: Models, reasoning, and inference* (2nd ed.). Cambridge University Press.