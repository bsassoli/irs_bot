## CHAPTER 4 Non-experimental investigation

### 4.1 PALEONTOLOGY AND NON-EXPERIMENTAL STUDIES

After reading this section, you should be able to:

- Describe how scientists have learned about woolly mammoth and mastodon life histories
- Give examples of when an experiment is impossible, impractical, and unethical
- List three varieties of empirical investigation that can be used when intervention is not possible

#### Prehistoric life histories

The woolly mammoth (*Mammuthus primigenius*) is an extinct species most closely related to today’s Asian elephants. It’s one of the best understood prehistoric animals, with research based on preserved bones and dung, depictions in prehistoric cave paintings, and even frozen carcasses found in Siberia and North America.

Research on woolly mammoths isn’t based on conducting experiments. Paleontologists can’t control the conditions encountered by woolly mammoths, of course, or recruit them into experimental and control groups. Indeed, they can’t intervene on woolly mammoths at all, since the last mammoths died millennia before the invention of science. Scientists can directly study some features of mammoths—namely, physical and chemical features of their preserved carcasses, skeletons, teeth, and dung—but they are also interested in other mammoth features that can’t be studied directly, like their diets, habitats, social interactions, and migration patterns. These features relate to woolly mammoth’s life history: the traits and circumstances that affected survival and reproduction of members of the species.

The impossibility of experimentation doesn’t mean paleontologists are at a dead end, unable to gain knowledge about woolly mammoth physical traits and life histories. Instead, they just have to get a little creative about where they find evidence and how they draw inferences.

For example, a 2021 article in the journal *Science* reported the results of chemically analyzing the tusk of a woolly mammoth that lived 17,000 years ago. The researchers compared the isotopes of two elements (oxygen and strontium) to geographic maps showing isotope variation in the environment. Because of how tusks grow over a mammoth’s lifespan, this comparison enabled the researchers to identify where this particular woolly mammoth moved across its lifetime. It was found to have a very extensive geographic range, traveling very long distances across what is now Alaska.

The researchers could also see that the mammoth had different types of movements across its lifespan. In the first 16 years of its life, it moved repeatedly among the same territories, which is similar to how herds of elephants move today, suggesting the mammoth was moving with a herd. Then, in the middle of the mammoth’s life, the isotopic variation increases, suggesting it wandered irregularly over long distances, which in turn suggests that it had left its herd, as mature males in elephant species also do. Finally, in the last year and a half of its life, the mammoth moved only within a very small area. Its tusk also showed an isotopic pattern associated with starvation, which is probably how it died.

In this study, researchers were able to use chemical analysis to determine where one mammoth lived and how this changed over time, and then to draw further conclusions about its health and even its social interactions—that is, whether it lived in a herd. These conclusions aren’t just about this particular mammoth, but about other woolly mammoths as well, and potentially other similar species, especially that also lived during the Pleistocene era. Indeed, other scientists followed up on this study by performing similar chemical analyses on the tusk of an American mastodon (*Mammut americanum*) that lived about 13,000 years ago. Mastodons and woolly mammoths both lived in the Pleistocene era and were similar in appearance, but mastodons are less closely related to modern elephants.  

##### Box 4.1 The roles of museums in science

Visiting a museum is a way to learn about history, art, and science in an enjoyable and leisurely way. But museums also play an important role in scientific research. To start, many have extensive collections of artifacts that are useful as objects of scientific research. These can include fossils, artifacts from human history, preserved plants and animals, and more. Usually, a given museum will only display small portions of its collection at a time; other items in the collection can be available for scientific research. Many science museums also have scientists on staff: curators often have advanced degrees in relevant fields of science and conduct their own scientific research. It’s also common for science museums to have partnerships with nearby universities. Scientists at the university may access collections or collaborate with curators, and sometimes graduate students participate in research activities at the museum as well. There’s a trend toward making this research activity visible to members of the public who visit the museum. Sometimes museums include discussion of ongoing related scientific research in their exhibits or have lab space in which scientists can conduct research while guests and visitors look on.

These scientists were able to identify how the mastodon’s migration followed annual seasons, including adult springs and summers in an unusual location far from its typical range; this was inferred to be a mating ground. Life history research on both woolly mammoths and modern African elephants was also deployed to support conclusions from their data, for example, that the mastodon lived in herds, and also to draw conclusions from their research to a broader group of similar large mammals. Thus, both of these studies analyzed remnants from particular, long-extinct animals with reference to ecological data and knowledge about modern‐day related animals in order to draw conclusions about long‐ago happenings that we can never observe directly, let alone intervene upon.

#### When experiments aren’t possible

In the paleontology research just described, the scientists couldn’t directly or indirectly control variables, intervene on, or even directly observe the phenomena under investigation. Experiments just aren’t possible. Instead, they brought together various forms of evidence to draw inferences about mammoth and mastodon life histories.

This is one reason experiments can’t be performed: sometimes it’s downright impossible to experiment on the phenomenon under investigation. The phenomenon may have occurred eons ago, like the life histories of Pleistocene mammals, or might occur far away, like star formation in nebulas, or it might be impossible to intervene on, like the Earth’s orbit around the Sun.

Other times, experimental intervention might technically be possible but is impractical or inadvisable. We might wonder what would happen to the solar system if the Earth’s moon exploded, but no one is rushing to develop the capacity to explode the moon in order to find out. And, though it’s important to research how childhood trauma impacts health in adulthood, it’s simply unethical to perform experiments to find out, as that would require randomly assigning children to the experimental condi‐tion of experiencing significant trauma.

In instances like these, scientists need to employ methods of empirical investigation that do not rely on experimentation. Some of these methods adhere closely to experimental methods, only deviating when necessary. For example, although scientists can’t randomly assign children to an experimental group that experiences significant trauma and a control group that does not, they can compare the later health outcomes of children who experience trauma to those who do not, attempting to ensure other variables do not differ consistently between the experimental and control groups. This is an observational study: data is collected and analyzed without performing interven‐tions and sometimes without aiming to control extraneous variables. Observational studies are especially common in studying human populations, as in the health sciences, educational research, psychology, and economics, as it is unethical or impractical to intervene on many circumstances of human life.

Scientists have developed numerous approaches to observational studies. These differ from experiments in a variety of ways and can use creative methods to manage extraneous variables. Other methods of empirical investigation deviate more fully from experimental methods. We have seen that paleontology research into mammoth and mastodon life history utilizes several different forms of evidence to piece together a picture of what things must have been like. This is a common approach to empirical investigation in the historical sciences, fields of science that investigate past events, such as archaeology, paleontology, and cosmology—the scientific study of the origin and development of the universe. Another category of empirical investigation relies on extensive data or simulation, such as computer simulations, as the basis for gaining knowledge when experiments aren’t possible or feasible.



##### EXERCISES

4.1 Recall: Describe how scientists have learned about woolly mammoth and mastodon life histories. What forms of evidence have they used? What kinds of conclusions have they drawn?

4.2 Think: Describe at least two ways in which the mastodon research described earlier built on the woolly mammoth research also described.

4.3 Recall: What are three reasons that scientists can not always perform experiments? Give an example of each.

4.4 Apply: Give an example of potential scientific research in each of the following categories:

- a. performing an intervention is impossible
- b. performing an intervention is impractical
- c. performing an intervention is unethical
- d. direct observation is impossible

4.5 Recall: List three varieties of empirical investigation available when intervention is not possible; briefly describe each.

4.6 Apply: Imagine you would like to know what your friend’s favorite restaurant is, but you can’t ask her directly. List 10 ideas for how you might get evidence bearing on this question. Put a checkmark next to the 4–7 ideas that you think would be the easiest evidence to obtain. Then, put an asterisk (*) next to the 4–7 ideas that you think would generate the strongest evidence. Write 2–3 sentences analyzing the potential kinds of evidence, the ease of collecting them, and their strength. Then, describe how you would approach your “study”—that is, which form(s) of evidence you would collect and how you would go about collecting it.

### 4.2 OBSERVATIONAL STUDIES

After reading this section, you should be able to:

- Describe and give an example of natural experiments, longitudinal studies, cohort studies, case studies, and phenomenological analysis
- Indicate three ways observational studies can manage extraneous variables
- Characterize the similarities and differences between each type of observational study and experiments

#### Natural experiments

Every now and then, nature gives rise to circumstances that are almost like an experiment. These so-called natural experiments occur when an intervention on an independent variable occurs naturally in real life, without scientists bringing it about. An example is the case of Louis Leborgne, who lost the ability to speak when he was about 30 years old. He could utter only a single syllable, tan, which he usually repeated twice in succession, giving rise to his nickname “Tan Tan.” Apart from his inability to speak, Leborgne exhibited no symptoms of physical or psychological trauma. He could understand other people, and his other mental functions were apparently intact. After Leborgne died at the age of 51 in a Paris hospital in 1861, the physician Paul Broca performed an autopsy. He found that Leborgne had a lesion in the frontal lobe of the left hemisphere of his brain, an area that came to be known as Broca’s area and recognized as essential to speech production.

Broca’s insight into brain activity was based on identifying accidental circumstances that altered speech production, which we can think of as the dependent variable, and then studying what was different in Leborgne’s brain that might give rise to the dramatic change. The function of Leborgne’s brain was not intervened upon by Broca; Leborgne just happened to suffer damage to a precise location in the brain such that Broca could later identify it as crucial for speech production. Broca conjectured that a specific area in the human brain is necessary for speech, from which he developed the expectation that an injury in Broca’s area causes the loss of speech—an expectation confirmed with an autopsy after Leborgne’s death. This inability to produce speech is now known as Broca’s aphasia.

Natural experiments also occur when groups just happen to get sorted accidentally—without any scientific intervention—into something approximating experimental and control groups. Some natural or historical process separates them out, such that one group but not the other can be construed as receiving an experimental treatment or condition. For example, in the early 2000s, researchers investigated how having women village council leaders, known as Pradhan, in India might affect social services. This was a natural experiment relying on a 1993 Indian constitutional amendment that required one-third of Pradhan positions to go to women. The law was structured so that the change in leadership was randomly implemented across villages, mimicking a surgical intervention. Researchers collected data on 265 village councils in West Bengal and Rajasthan, including the minutes of village council meetings and Pradhan interviews. They also collected data about social services and infrastructure in each village and requests that had been submitted to the village council. The Pradhan’s policy decisions and villagers’ requests were unaffected by their interactions with the experimenters, since those requests and decisions were already made during data collection. The researchers found that women policymakers had important effects on social service policy decisions. Women Pradhan increasingly invested in the social goods that were more closely connected to women’s concerns in a village: drinking water and roads in West Bengal and drinking water in Rajasthan. They invested less in public goods connected to men’s concerns: education in West Bengal and roads in Rajasthan.

As this example illustrates, governmental policy can support natural experiments or at least indirectly control some variables in an observational study. Many studies examining the impact of the Covid‐19 pandemic and pandemic policies like stay‐at‐home orders or remote school instruction relied on variation in policy to create quasi‐experimental conditions.

For example, an observational study evaluating pandemic‐related learning loss in elementary‐school students compared the standardized test scores of Ohio third graders in school districts that began the 2021 school year in remote instruction, hybrid instruction, and in‐person instruction. This study benefits from both direct and indirect variable control from law and policy. All Ohio third graders are required by law to take the same standardized test at the same time of year. But, school districts across the state varied in when they returned to in‐person instruction—from the start of the 2021 school year in August to spring of 2022. That variation enabled researchers to determine whether additional time in remote instruction extended learning loss. And it did: all districts saw a decrease in student proficiency, but 11.2% fewer students who began the year with remote instruction met the state benchmark for advancing to fourth grade, whereas 6.5% fewer students with hybrid instruction and 5.3% fewer students with in‐person instruction met the benchmark to advance.

#### Managing extraneous variables

As stressed in Chapter 3, experimental methods are designed to eliminate the possibility of confounding variables through direct and/or indirect variable control. The same principles apply in observational studies, though researchers must seek circumstances that naturally hold extraneous variables constant (direct control) or vary them randomly (indirect control), or they must use techniques to manage extraneous variables when direct and indirect control aren’t possible. Circumstances sometimes perfectly mirror experimental conditions, giving rise to natural experiments, but it is common in observational studies for there to be extraneous variables that circumstances do not directly or indirectly control.

Scientists have developed various methods to manage uncontrolled extraneous variables in observational studies. For example, in the study on Covid‐19 learning loss just mentioned, school districts were not randomly distributed across the conditions of remote, hybrid, and in‐person instruction. So, researchers used the data to estimate the impact of other factors like unemployment and poverty level on learning loss and then they calculated how that effect would impact remote versus hybrid versus in‐person districts. They found that those extraneous variables did not account for the difference observed.

Another method sometimes used to help account for extraneous variables is to match subjects with different characteristics across the different conditions, and then only investigate outcomes for those subjects. For example, if this method had been employed in the Covid‐19 study just discussed, researchers would identify individual third graders in remote, hybrid, and in‐person instruction and attempt to match them with similar third graders across these instructional formats, attending to potential confounding variables like income level, race, prior academic achievement, and family unemployment or illness. This method isn’t as successful at controlling variables as random group assignment, but it goes some way toward correcting for confounding variables (as long as researchers are already aware of them).

Other approaches to accounting for extraneous variables make use of the passage of time. In a longitudinal study, observations are made of the same variables over time, in many cases over a long period of time. The study on Covid‐19 learning loss we have been discussing is a longitudinal study: the researchers compared standardized test performance of all third graders in Ohio public schools in 2022 with the performance of third graders in Ohio public schools in prior years. Unlike all prior students, third graders in 2022 had experienced educational disruptions from Covid‐19, and there’s good reason to think Covid‐19 and its downstream effects are the only major differences between these third graders and those of prior years.

Longitudinal studies can be carried out over many years. The Early Childhood Longitudinal Study was started by the US Department of Education in the late 1990s and has followed 20,000 American children, examining their development, performance at school, and early school experience. Researchers also conducted extensive interviews with the subjects’ families. This study provides a lot of information about American children’s development and family life, such as what is important about kindergarten experiences for later academic success, and whether offering Algebra I to eighth graders as an online course is effective. (It is.) Early Childhood Longitudinal Studies continue, with the latest following the kindergarten class of 2023–2024 through the fifth grade.

One type of longitudinal research is a cohort study, where researchers select a group of subjects sharing some defining trait and study them over time, in comparison to another group of subjects that is as similar as possible except without this trait. Cohort studies can reveal changes over time in the characteristics of the group of subjects with the trait of interest. For example, subjects with Covid‐19 may be grouped according to demographics like age and gender, or treatments they received, and then have their health outcomes assessed.

# Case studies

One form of observational study that is quite different from a controlled experiment is a case study, a detailed examination of a single individual, group, system, or situation in a real‐life context. Case studies allow researchers to gain a first‐hand understanding of a phenomenon as it occurs in its specific context. Often, various sources of data are used for case studies. Depending on the phenomenon under investigation, these may include observations of a person’s daily routine, unstructured interviews with participants and informants, letters, e‐mails, social media activity, legal or archival records, buildings, animal or plant behavior, and so forth.

A defining feature of case studies is their reliance on qualitative data. Qualitative data consist of information in non‐numerical form, whereas quantitative data are in numerical form, which makes them easily comparable. For example, “the subject reports being very angry” and “the subject reports being not angry at all” are qualitative data, whereas “the subject reports their anger is a 3 on a scale of 1–5” and “the subject reports their anger is 1 on a scale of 1–5” are quantitative data. Case studies are often employed in the context of qualitative research in epidemiology, psychiatry, education, ethnography, archaeology, and other social sciences.

Case studies may focus on an instance of a common situation or condition, or they may focus on outliers, that is, individuals or situations that deviate from what’s common. An example of a case study on a common situation is the research reported in the 2013 book Paying for the Party by education researcher Elizabeth Armstrong and sociologist Laura Hamilton. Armstrong and Hamilton conducted a five‐year study focusing on the college experience of one group of women at a large, public university in the United States. Their research reveals ways in which social and academic life at such a university prioritizes a “party pathway” catering to affluent and well‐connected students but disadvantages less affluent and first‐generation students, to the detriment of their education and future careers.

A downside to case studies is that they offer no ways to control extraneous variables or to compare a variety of outcomes. Because the research focuses on only one individual, event, or group, results can be difficult to replicate and to generalize. Case studies are also vulnerable to bias due to the evaluation of qualitative data and no blinding.

Yet, in some fields, detailed first‐person, qualitative reports of individual cases play important roles without necessarily aiming for reproducibility or generalizability. For example, clinical case reports often provide clinicians with accounts of surprising or novel conditions in an individual patient, which may generate contextually situated understanding of those conditions and new hypotheses about diagnosis and treatment. Case studies can also be valuable exploratory research, as a way to indicate promising foci for later experiments or more robust observational studies. For example, Armstrong and Hamilton’s research may set the stage for a broader investigation into how a university Greek system or certain majors differently affect students with differing socioeconomic status.

### Phenomenological analysis

Case studies sometimes reveal a first‐person perspective on some phenomenon or situation, that is, how things feel or seem to the subject of the investigation. You may wonder whether such subjective accounts can be of scientific value; after all, science involves empirical investigation and comparison across individual perspectives. It’s true that there is oftentimes scientific value in generalizing from one individual’s perspective. But it’s also true that first-person perspective on situations a person is experiencing can be essential to scientific knowledge. First-person accounts of lived experiences play various roles in fields such as psychiatry, health sciences, sociology, and anthropology.

First-person accounts sometimes are called phenomenological to emphasize that they are grounded in the methods of phenomenology, a philosophical tradition concerned with making sense of embodied subjective experience. In science, phenomenological analysis aims to describe and analyze what some experience is like for a particular individual. Phenomenological analysis is focused on subjective experience. It simply makes no sense to say that one’s experience of, say, pain is incorrect if it varies from others, though we might contrast common experiences of pain with outlying experiences of pain.

##### Box 4.2 Phenomenological analysis

Phemenology is a tradition in philosophy that originated in Europe in the early 20th century. It includes the work of philosophers like Edmund Husserl, Martin Heidegger, Edith Stein, Maurice Merleau-Ponty, Aron Gurwitsch, and others. The etymology of the word phenomenology comes from ancient Greek and means study of that which appears. What unifies strands of research within the phenomenological tradition is their focus on the structures of consciousness as experienced from the first-person perspective. One central structure of conscious experience is its intentionality—that is, being about or of something.

Three methods in phenomenology to study the structures of conscious experience can be relevant to scientific fields like cognitive science, psychiatry, medicine, anthropology, and sociology. The first method consists in describing conscious experience as it is experienced, without influence by scientific or historical interpretation. Another method consists in interpreting conscious experiences by situating them in their social, material, and technological context. A third method consists in analyzing the essence of and conditions for different kinds of intentional states like perceptions, emotions, beliefs, and desires, pointing out their differences and similarities in relation to time, space, self, body, and interpersonal social relationships.

Phemenological analysis has been used in psychology and neuroscience to distinguish actions of our body that we feel a sense of agency over from mere movements we are not responsible for. This distinction is salient when you are, say, pushed or jostled: you experience your body moving but do not experience yourself as having performed that movement. In light of this distinction, psychologists and neuroscientists have designed experiments to investigate whether there are different mechanisms. responsible for these experiences. These experiments might help our understanding of symptoms of psychiatric conditions like the experience of thought insertion in schizophrenia, where an individual does not feel agency over some of their thoughts. In this example, phenomenological analysis helps clarify two distinct phenomena, which scientists can then probe with experiments and also compare with psychiatric patients’ accounts of their own experiences.

Phenomenological analyses can also play a role in medicine, particularly in relation to what it is like to experience illness and how healthcare providers communicate with patients. For example, phenomenological insight can inform the development of methods for better understanding a patient’s experience that go beyond verbal reports. Methods that tap into the experience of illness as an embodied phenomenon that affects one’s experience can yield insight into changes in movements and behaviors. This can help bridge the gap between patients’ experience of illness and healthcare providers’ understanding of the illness and, accordingly, improve patients’ trust in their doctors.

##### EXERCISES

4.7 Recall: Give a brief description and an example of natural experiments, longitudinal studies, and cohort studies, and then describe how each manages extraneous variables.

4.8 Apply: Look back at this section’s description of the study of standardized test scores of Ohio third graders in school districts in 2022, after educational disruption from Covid-19. Describe how direct and indirect variable control were each used in the study (without researcher intervention), how the researchers managed the extraneous variables of unemployment and poverty level, and why being a longitudinal study was important for variable control.

4.9 Recall: Give a brief description and an example of case studies and phenomenological analysis.

4.10 Think: Why are qualitative data essential for case studies and phenomenological analysis?

4.11 Apply: For each of the following hypotheses, decide whether it’s best investigated with an experiment or observational study and, if the latter, which variety of observational study (natural experiment, longitudinal study, cohort study, case study, or phenomenological analysis). Explain your reasoning, taking into account the hypothesis under investigation, the feasibility or ethics of experimentation, and other constraints.

- a. Whether rising interest rates make stock prices go up
- b. Whether men’s resumes are judged more positively than women’s, regardless of the resume content
- c. Whether Covid-19 patients admitted to a hospital sooner are less likely to die
- d. Whether a patient who is fidgeting feels greater pain (regardless of her reported pain level)
- e. Whether a new medicine is effective in decreasing symptoms of Parkinson’s disease

4.12 Think: Describe the example of phenomenological analysis in psychology and neuroscience or the example from medicine. In the example you’ve described, what do you think is added by considering a person’s subjective experience?

### 4.3 IMAGINATION AND COMPUTATION

After reading this section, you should be able to:

- Describe and give an example of thought experiments and computer simulations
- Define big data and machine learning and describe how these are used in scientific research
- Analyze the advantages and disadvantages of each of these non-experimental approaches

#### Thought experiments

The power behind experimentation is intervention and variable control. Observational studies employ a variety of different techniques to approximate this combination of intervention and variable control. Another way to proceed when direct or indirect variable control needed to run an experiment isn’t possible is to perform an intervention on something else more under our control. An opportunity for this is provided by our rich imaginations.

Thought experiments involve an imagined intervention on an imagined system to learn about the role of the independent variable in the real world. Thought experiments may supplement empirical investigation or, in some cases, can replace it entirely. In the right conditions, your imagination can be a reliable guide to learn about reality.

Here’s a simple example. Someone who does not often drink in bars stays out late one night in a bar with their friends. They awake in the morning feeling physiologically and psychologically poor. They might ask themselves how they would feel if, instead, they’d gone home earlier from the bar. They know enough about the circumstances and their own propensities to be able to infer that they’d have had fewer drinks, slept a few more hours, and now would be less dehydrated and better rested. They can’t go back in time to actually perform this intervention, and so they use this informal thought experiment to arm themselves with knowledge about how staying out late drinking in bars rather than going home to sleep (the independent variable) affects their physiological and psychological health the next morning (the dependent variable).

In science, thought experiments can be used to test a hypothesis, to show that nature does not conform to one’s previously held expectations, or to suggest ways in which expectations can be revised. In 16th‐ to 17th‐century Italy, Galileo Galilei used many thought experiments in his investigations of physics and astronomy. In one instance, he wished to investigate an idea shared by many “natural philosophers” of his time that objects with different weights fall at different speeds. Galileo asked his readers to assume that this was true: that heavier objects fall faster than lighter objects. He then imagined two objects, one light and one heavy, connected to each other by a string and dropped from the top of a tower. If the heavier object fell faster, then the string would pull taut. But, Galileo reasoned, both objects together are heavier than the heavy object on its own. So, the two objects together should fall faster than either object alone. The objects cannot simultaneously fall both faster and slower, and so the idea that was the starting point for this thought experiment could not be right. Galileo thus conjectured that the speed of a falling body is not dependent on its weight.

Newton also used thought experiments to help show how his theory of gravitation worked. He had readers imagine a cannon at the top of an extremely tall mountain and then asked: what would happen if somebody loaded the cannon with gunpowder, and then fired? Plausibly, Newton reasoned, the cannonball would follow a curve, falling faster and faster because of gravity’s force, and would hit the Earth at some distance from the mountain. But what if one used more gunpowder? The velocity of the cannonball would be greater, and it would travel farther before falling back to Earth following a curve trajectory. But if one used vastly more gunpowder, then, Newton suggested, the cannonball would travel so fast that it will fall all the way around the Earth, never landing. The cannonball would be in orbit, going around again and again just like the Moon! If the cannonball went even faster, then it would escape Earth’s gravity heading out in space. Newton’s theory of gravitation provided the resources to arrive at these same conclusions through mathematical calculations. Yet, imagining this situation gives a satisfying, intuitive sense for how an object like the Moon can stay in orbit by remaining in constant free fall.

In the 20th century, thought experiments were central to the development of both transformative theories in physics: the theory of relativity and quantum mechanics. An example that’s received some popular attention is the thought experiment about Schrödinger’s cat. Quantum mechanics treats subatomic particles mathematically as if they are not in discrete locations but probabilistically spread over possible locations, and there’s a longstanding and unresolved debate about how to interpret this mathematical characterization. When a technological apparatus is used to detect an electron, it is always in a single, discrete location. Are subatomic particles like electrons really not in discrete locations until they are measured? If so, what changes when they are measured?

Erwin Schrödinger, Nobel Prize‐winning physicist, asked us to imagine a cat, a readily observable living entity, in an apparatus that entangled its state with a subatomic particle such that the cat is killed if some quantum event occurs and not otherwise. Yet, quantum mechanics gives us just probabilities for whether the event occurs. If this is truly the state of the particle, then the cat is not definitively alive or definitively dead but in a probabilistic state between these. This result is inconsistent with our experience of reality, of the possible ways cats could be, and is meant to show that an answer is needed for how the mathematics of quantum mechanics should be interpreted for everyday experiences of reality. Note that this thought experiment was not intended as a challenge to quantum mechanics, which has significant evidence in its favor, but to demonstrate the need to offer an interpretation for how that theory relates to our everyday observations

Just like experiments, thought experiments may suffer from poor experimental design or from scientists inferring unjustified conclusions from them. An additional, very significant limitation is that experimental results are always limited by scientists’ powers of imagination. The world can surprise us, including in experimental results, in ways that thinking through the implications of ideas generally cannot. One possible response to the challenge of Schrödinger’s cat, for example, is just to point out that our imagination may be ill‐equipped to comprehend what we have never directly experienced, like the behavior of subatomic particles.

#### Computer simulations

As thought experiments can be used to imagine fictive interventions, computers can be used to simulate interventions. Computer simulations are computer programs developed from data to mimic a phenomenon. Examples of simulated phenomena might include ecological effects of global climate change, likely shapes of galaxies after two collide, and the progression of a pandemic under different circumstances.

Computer simulations play key roles in many fields of contemporary science, including climate science, astronomy, and epidemiology (the fields of the computer simulations used as examples in the previous paragraph). Computer simulations may be used in combination with experiments or observational studies, or they may be used when experiment or observational studies are not possible. Regardless, a significant amount of background knowledge about the phenomenon is required in order to develop computer programs that can adequately simulate the behavior of a phenomenon. This limitation is similar to the limitations of thought experiments: computer simulations are only as accurate as the data they are based on and how they are programmed.

By running computer simulations and intervening on their features, scientists can learn what would happen to a phenomenon in different circumstances or explore particular features. For example, computer simulations of the Earth’s climate use dynamical equations to model the interactions of solar energy, chemicals in the atmosphere, ice, and other factors over time. These simulations can then be studied to yield insight into how climate change is progressing, its variable impact on temperature and weather in different parts of the world, and the potential of different changes to slow or reverse its progression.

Computer simulations allow researchers to have extensive control over extraneous variables. After all, in computer simulations, the researchers decide what data to base the simulations on and what factors to take into account in the program. Computer simulations often can be easily evaluated for their degree of external experimental validity, in the sense that we can evaluate how well their results generalize from the contrived conditions to real‐world phenomena by comparing the results of the simulations to empirical measures of the phenomena. To the extent the simulation and the real‐world system it describes have relevant similarities, we can be more confident the computer simulation produces trustworthy, generalizable results.

There was more public attention to computer simulation in science during the Covid‐19 pandemic than perhaps at any previous time. Epidemiologists began to use computer simulations very early in the pandemic to predict rates of transmission, hospitalization, and mortality and to influence policy decisions on stay‐home orders and school and business closures. Early in the pandemic, these simulations were by necessity based on limited data that were rapidly developing as researchers learned more about the SARS‐CoV‐2 virus, its transmission, and its effects. Models provided broad ranges of possibilities for pandemic progression, and different models often varied radically in their predictions. Yet sometimes these limitations were overlooked or brushed aside, and scientists, policymakers, or the public interpreted the models as making highly specific predictions and different models as providing contradictory predictions.

This instance of computer simulation in science was typical in the sense that early simulations of novel phenomena—like exploratory experiments or early scientific investigations of other kinds as well—begin with some guesswork and involve iterative improvement. But this instance was highly atypical in the extensive publicity received by early predictions of computer simulations. Because that publicity was so unusual, there is a risk that people who don’t ordinarily pay attention to computer simulation in science will infer from its limitations early in the Covid‐19 pandemic that it is an untrustworthy technique. As we have noted, computer simulations are limited by the existing knowledge used to develop the computer program, as well as by what factors scientists include or exclude from the simulation.

#### Big data and machine learning

Experiments ultimately are valuable because they generate data and because we know certain things about the conditions in which the data were generated. Observational studies, thought experiments, and computer simulations seek to approximate this approach or seek other kinds of data. A very different approach is to simply maximize data and see what you can learn.

Big data are very large data sets that cannot be easily stored, processed, analyzed, or visualized with traditional methods. Big data sets can reveal unexpected patterns, trends, and associations. Scientific researchers use big data to study weather patterns, DNA, and risk factors for disease, among many other phenomena. Cameras, medical devices, e‐mails, social media, and the internet, have produced an ever‐increasing stream of data in recent years. Some scientific investigations also produce a tremendous amount of data to be processed and analyzed, especially those relying on computer technologies such as telescopes in astronomy and the Large Hadron Collider (LHC) at CERN. For example, if all the sensor data in LHC were recorded, this would approach 500 exabytes per day, or 500 quintillion (5 × 1020) bytes. And, the processing of scientific data has also gotten much faster. While it originally took ten years to sequence the human genome, completed in 2003, a genome can now be sequenced in less than one day.

Generally, an algorithm is a procedure for obtaining some outcome that halts in a finite number of steps. Machine learning algorithms in particular are step‐by‐step procedures run on powerful computers that enable scientists to mine large data sets for patterns or to perform other tasks. Machine learning algorithms have begun to play numerous roles in science, as well as business and our everyday life. When, for example, you search the internet, access social media, or order food online, there is some learning algorithm operating in the background that has learned how to rank web pages, make personalized suggestions about content you will probably like on social media, and coordinate food orders and timely deliveries. Once the algorithm learns what to do with data, it will perform such specific tasks automatically.

In scientific research, machine learning algorithms can mine data for patterns, process images and audio files, and make predictions. For example, imagine that you want to determine general trends in food preferences, and you have at your disposal a data set containing all social media posts produced in the past year. Filtering those posts to a subset relevant to food preferences is extraordinarily valuable, as is visualizing the data about the popularity of various foods. The patterns and trends uncovered by analyzing a large amount of data about some subject can give insight into relationships among variables of interest and can be used to make predictions.

Big data and machine learning can help limit the influence of researchers’ expectations on findings. For example, current classifications of mental illness, such as schizophrenia, are often criticized for being too coarse-grained and ill-defined to help clinicians and therapists make a reliable diagnosis and provide patients with adequate treatment. Because psychiatric classifications lump together several different psychiatric symptoms and variables that may have little in common, studies relying on machine learning for mining large sets of behavioral, biological, and other types of data from many patients and healthy subjects (as a control group) might discover more useful ways of classifying psychiatric conditions, even if these classifications are not intuitive to us.

The analysis of big data can even help us better understand how science works. For example, bibliometric methods, including the analysis of networks of citations in published work, can be used to investigate the level of productivity of a certain field of research, trends in the topics of scientific research, and even the social dynamics underlying scientific practice. The number of citations of a published article is an index of recognition, which is one of the primary rewards for scientists. So, citation rates and patterns can be used to quantify scientific impact and to predict what factors might affect the course of science.

But studies relying on big data and machine learning also raise distinctive challenges. One challenge is their opacity. The algorithms used in machine learning applications are sometimes unknown to outside researchers, because of security, business, or copyright reasons. This goes against the culture of open exchange that is typical of the social institution of science. Beyond this, how and why an algorithm returns a certain output—for example, a certain mental illness classification decision—can be difficult or impossible to explain even for the researchers who designed it. Finally, even the data on which the algorithm is trained may be unknown to the users of the algorithm.

This opacity is especially problematic because outputs or the underlying data may be misleading or biased, or researchers’ assumptions that influenced the algorithm may be wrong. In 2008, researchers from Google claimed that they could immediately predict what regions experienced flu outbreaks based simply on people’s online searches. The assumption was that when people are sick with the flu, they often search for flu-related information on Google. Unfortunately, this idea wasn’t borne out. Google Flu Trends made very inaccurate predictions, significantly overestimating flu outbreaks, and was shut down.

Another challenge with big data and machine learning techniques is how they may inherit researchers’ biases, an outcome they were designed to avoid. This is a negative result for scientific investigation in general, but it is especially problematic when the biases in question reflect sexism, racism, and other prevalent societal prejudices with actual real-life consequences. A prime example is a machine learning algorithm developed by the company Amazon in 2014 for employee recruitment. The algorithm was trained to review job applications automatically, indicating which applicants would be the best to hire. The algorithm had the unintended effect of systematically ranking male applications higher than female applications. Investigation revealed that the algorithm was trained on historical data about the performance of past employees at Amazon, and these employees were predominantly men. Because it was trained on this data set, the algorithm predicted that high‐performing employees would likely be men. When Amazon researchers realized this in 2018, they abandoned the hiring approach. This example illustrates how the opacity of machine learning algorithms can have unintended consequences with a real impact.

##### Box 4.3 Algorithmic fairness and justice

In many domains like business, law, insurance, police work, and healthcare, human decision-making is increasingly supported—and sometimes replaced—by machine learning algorithms. These algorithms attempt to compute predictions based on historical data on which the algorithms were trained. Algorithmic predictions can be objective, but they can also unwittingly reflect unfairness and human prejudices without any deliberative effort by programmers to include these in the algorithm. For example, Tay was a chatbot released by Microsoft Corporation in 2016, and shutdown shortly thereafter because it produced sexist and racist content on Twitter. COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) was used by US courts to predict criminals’ risk of recidivism, but it produced a higher rate of false positives (non-recidivists incorrectly labelled “high risk”) for Black defendants than for White defendants and a higher rate of false negatives (recidivists incorrectly labelled “low risk”) for White defendants than for Black defendants. Cases like these raise difficult ethical and political questions, often grouped under the headings of algorithmic fairness, algorithmic justice, or algorithmic bias. One question concerns the right criterion of fairness—when it comes to recidivism, for example, should the criterion be equal rates of false positives and false negatives between two given groups, or should it be absolutely equal predictive accuracy across all individuals? A second question is whether algorithms are at least less biased than human decision-making without algorithms. Another question is whether, and why, algorithmic justice should be informed by secular, egalitarian, and liberal values rather than, say, Confucian values or politically conservative values.

##### EXERCISES

4.13 Recall: Watch Video 8 Describe and give an example of a thought experiment and a computer simulation.

4.14 Think: Describe the similarities and differences between thought experiments and computer simulations.

4.15 Recall: Define big data and machine learning algorithm and describe how these are used in scientific research.

4.16 Think: Describe how each of thought experiments, computer simulations, and machine learning algorithms are useful to gaining scientific knowledge. For each, what are the limitations in their usefulness?

4.17 Think: Describe three advantages and three disadvantages of scientific studies relying on big data and machine learning algorithms.

4.18 Apply: Algorithms and big data have an ever-increasing impact on our daily activities. Consider the following activities:

- a. Online dating
- b. Autonomous vehicles
- c. Police profiling
- d. Online trading
- e. Urban planning

Choose three of these activities, then, for each of the three, write a short paragraph describing one task an algorithm might perform and what type of data the algorithm might be trained on to learn that task. Then, formulate a question or concern about the behavior of the algorithm in that domain, explaining its importance. What reasons or values are relevant to addressing the question or concern?

### 4.4 MULTIPLE SOURCES OF DATA

After reading this section, you should be able to:

- Describe the steps of meta‐analysis and how meta‐analysis improves experimental validity
- Define methodological omnivory and indicate the circumstances in which it’s useful
- Characterize how methods of empirical investigation depend on the phenomenon under investigation, the circumstances, and the aims

#### Meta-analysis

As we’ve seen, observational studies employ various methods to control or account for extraneous variables. Nonetheless, many studies have limited utility. There may be fewer subjects than needed, confounding variables, no control group, or other foibles that limit their explanatory power. But experiments can have some of these same downsides, too; as emphasized in Chapter 3, most experiments cannot adhere to ideal experimental procedures.

The technique of meta-analysis offers a way to combine the results of multiple experiments or observational studies of the same hypothesis to strengthen the conclusions that can be inferred. The idea is that several studies, each with different limitations, can be combined to additionally account for extraneous variables and correct for other shortcomings. Meta‐analysis is thus used to better estimate the real extent of the independent variable’s influence on the dependent variable by combining the different findings of several distinct studies. Meta‐analysis can also identify patterns in study results, including helping to reveal and analyze possible reasons for conflicting results.

Meta‐analysis is especially common in healthcare research, but it is increasingly employed in other fields as well. To conduct a meta‐analysis, researchers first identify a question that has been targeted in existing scientific research—for example, whether and to what extent patients experience the placebo effect when they know they are receiving a placebo. (See Chapter 3 on the placebo effect.) Then, researchers conduct a literature search and decide which studies should be included in the meta‐analysis.

For each included study, an effect size (a quantitative measure of the strength of a phenomenon) is estimated from the study’s results, and then the effect sizes are combined using statistical methods that are beyond the scope of this book. This results in an estimate of the common effect size across the studies and also a measure of how much the study outcomes deviate from one another.

In the meta‐analysis of the placebo effect, researchers screened 1,246 studies and selected 11 to analyze. These included randomized controlled studies of any medical condition or mental illness that compared administration of a placebo with patients’ knowledge to no treatment at all. The application of relevant statistical methods resulted in the finding of a large overall effect size but also high deviation in effect sizes across studies. The studies included in the meta‐analysis assessed the placebo effect on improving symptoms from back pain, fatigue from cancer, attention deficit hyperactivity disorder, nasal allergies, depression, irritable bowel syndrome, and hot flashes from menopause.

From the large overall effect size, the researchers conclude that administering placebo medications, even with patients’ knowledge that they are receiving placebos, may be a promising treatment for several difficult‐to‐treat medical conditions and mental illnesses. The researchers also note that more research is required, especially regarding efficacy of the placebo effect for different conditions, how patients are notified of the placebo, and how patient expectations influence the treatment. Comparison across studies included in the meta‐analysis is used to identify these potential sources of variation to target for further study.

Meta‐analyses offer a way to consider and integrate the results from many existing studies, thereby increasing the knowledge gained from them and identifying sources of discrepancies in their results. Sources of discrepancies can include differences in the specific phenomena investigated (such as which health condition patients have), the experimental design (such as how patients are notified about the placebo), techniques of data analysis, and confounding variables (variation in patients’ mindset or expectations). All of this can be used to improve internal experimental validity by accounting for confounding variables as well as external experimental validity, including both ecological validity, through variation in study circumstances, and population validity, through variation in subject inclusion across studies. (See Chapter 3 for more on internal and external experimental validity.)

There are also some drawbacks and limitations to meta‐analysis. First, the results of meta‐analyses inherit any systematic flaws of the studies they combine. It can also be  difficult to control the influence of researcher bias, as there are unavoidable judgment calls in which studies to include in the analysis and whether some study has flaws that warrant its exclusion. Finally, the whole point of a meta‐analysis is to combine studies with different specifics and different findings to see what they reveal together.

The measure of deviation across study outcomes can identify the extent to which findings of individual studies vary, but beyond this, meta‐analysis ignores differences across studies that may provide important information. For example, it could be that the placebo effect is an especially promising treatment for some conditions but not others. Focusing on the strength of the placebo effect across a wide variety of medical conditions and mental illnesses may lead researchers to overlook or insufficiently attend to variation across these conditions.

#### Methodological omnivory

Recall the paleontology research into woolly mammoth and mastodon life histories from the beginning of the chapter. Experiments on living mammoths and mastodons simply aren’t possible, and scientists can’t conduct observational studies of these prehistoric creatures either. The targets of investigation are separated from the investigators by eons; they can’t observe behavior or collect direct evidence. Instead, as we saw, paleontologists creatively employed techniques of chemical analysis on the tusks of these extinct creatures with reference to ecological data and knowledge about modern‐day related animals in order to draw conclusions about long‐ago happenings that we can never observe directly.

Philosopher Adrian Currie has dubbed this approach in the historical sciences methodological omnivory, which is the use of multiple methods and specially tailored tools to generate evidence about specific targets. This approach is identifiable from scientists combining a number of distinct methods, often from multiple scientific fields, and also from significant investment in developing special tools tailored to specific evidential roles.

For example, the mammoth and mastodon studies used highly specific isotope analyses of layers of a tusk and extensive measurements of isotope ratios in the surrounding geologic formations to piece together a specific animal’s movements through its range during the different parts of its life. Comparison with migration and life history patterns of modern elephants enabled the researchers to draw other inferences from the animal’s movements, such as whether it moved with a herd and that it annually travelled long distances to a mating ground. These investigations were also supported by other past research into mammoth and mastodon physiology, lifespan, and more. Across studies like these on related phenomena, paleontologists can piece together more and more evidence to develop increasingly extensive knowledge of long‐ago creatures.

Another technique involved in the mastodon study was the development of a computer simulation that used the isotope data to calculate likely distances travelled and geographic locations. As this illustrates, computer simulations can be an important feature of methodological omnivory, as simulations are one way of drawing broader inferences from the data scientists are able to assemble.

When experimental data aren’t readily available, a variety of techniques from observational studies to computer simulations, big data, and meta-analysis are available for potential use. As we’ve seen, there’s no single answer to which of these methods is best. There are different recipes; and which ones are effective depend on the phenomenon under investigation, the circumstances of investigation, and the aims of the research.

Depending on the circumstances of the investigation, some phenomena lend themselves to data collection in quasi-experimental circumstances or to other forms of direct and indirect variable control. Sometimes, when ecological validity is important, these types of observational studies are better than experiments. Variability in a phenomenon and prioritizing first-hand experience increases the value of case studies and even phenomenological analysis. Thought experiments and computer simulations can provide indirect access to features of phenomena not available for experimental manipulation. And when phenomena produce a vast store of data, big data techniques and machine learning algorithms can be useful. If many empirical studies already exist, a meta-analysis of those studies can be more useful than direct empirical investigation. Finally, some phenomena are so distant in time or space that they can only be studied very indirectly, using various tools of simulation and studying their distant effects, as in paleontology and astrophysics.

##### EXERCISES

4.19 Recall: Describe the steps of meta-analysis. Then, describe how a meta-analysis can improve both internal and external experimental validity.

4.20 Apply: Find a published meta-analysis and look over the article (or, your instructor may provide one for you to analyze). Summarize the study, including how many studies were included, how the researchers selected those studies, if any were subsequently excluded, what conclusions they reached, and any concerns or open questions they indicated.

4.21 Recall: Define methodological omnivory and describe how the paleontology research into woolly mammoths and mastodons in section 4.1 illustrates its use.

4.22 Apply: Describe a new example of scientific research where methodological omnivory is used. What about this research and the circumstances in which it’s conducted make methodological omnivory useful? (Hint: you might consider research in a historical science, like anthropology or cosmology.)

4.23 Recall: List all the kinds of empirical investigation without experiments we’ve discussed in this chapter. (By our count, there are 11.) For each, briefly describe how it generates empirical evidence and in what kinds of circumstances it’s useful.

##### FURTHER READING

For a concise treatment of qualitative research and its methodology, see Golafshani, N. (2003). *Understanding reliability and validity in qualitative research*. The Qualitative Report, 8(4), 597–606.

For more on the role of thought experiments in science, see Horowitz, T., & Massey, G. (Eds.). (1991). *Thought experiments in science and philosophy*. Rowman & Littlefield.

For more on the use of big data in science, see Leonelli, S. (2020). “Scientific research and big data”. In E. N. Zalta (Ed.), *The Stanford encyclopedia of philosophy* (Summer 2020 ed.). https://plato.stanford.edu/archives/sum2020/entries/science‐big‐data/.

For an introduction to phenomenology in philosophy and cognitive science, see Kaufer, S., & Chemero, A. (2015). *Phenomenology: An introduction*. Polity.

For more on meta-analysis and how it varies across fields, see Kovaka, K. (2022). “Meta-analysis and conservation science”. *Philosophy of Science*, 89(5), 980–990.

For a discussion of methods in the historical sciences and an articulation of “methodological omnivory,” see Currie, A. (2018). *Rock, bone, and ruin: An optimist’s guide to the historical sciences*. MIT Press.